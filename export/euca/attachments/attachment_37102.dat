Started by upstream project "dev-test-regression-test" build number 10408
originally caused by:
 Started by upstream project "Quick Launch Maint-4.4" build number 97
 originally caused by:
  Started by user Swathi Gangisetty
[EnvInject] - Loading node environment variables.
Building remotely on Tester-CentOS7 (i-02604879) (qa chefdk012 deployer) in workspace /mnt/swathi/dev-swathi-EUCA-12910/97
[97] $ /bin/sh -xe /tmp/hudson1302021991609630162.sh
+ '[' -f archive.zip ']'
[97] $ /bin/sh -xe /tmp/hudson3605969873312989440.sh
+ curl http://git.qa1.eucalyptus-systems.com/qa-repos/nephoria/raw/master/toolbox/create_nephoria_env.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  1763    0  1763    0     0  23858      0 --:--:-- --:--:-- --:--:-- 24150
+ chmod 755 create_nephoria_env.sh
+ ./create_nephoria_env.sh -r git@git.qa1.eucalyptus-systems.com:qa-repos/nephoria.git --adminapi-repo git@git.qa1.eucalyptus-systems.com:qa-repos/adminapi.git -n master -a master -v envnephoria
Fetching origin
origin	git@git.qa1.eucalyptus-systems.com:qa-repos/adminapi.git (fetch)
origin	git@git.qa1.eucalyptus-systems.com:qa-repos/adminapi.git (push)
* master
  remotes/origin/HEAD -> origin/master
  remotes/origin/eucanetd
  remotes/origin/master
  remotes/origin/mido_filter_info
  remotes/origin/newsetup
  remotes/origin/nocert
Already on 'master'
From git.qa1.eucalyptus-systems.com:qa-repos/adminapi
 * branch            master     -> FETCH_HEAD
Already up-to-date.
/mnt/swathi/dev-swathi-EUCA-12910/97
/mnt/swathi/dev-swathi-EUCA-12910/97
total 144
drwxr-xr-x 21 root root 4096 Dec  8 23:37 .
drwxr-xr-x  3 root root 4096 Dec  8 22:41 ..
drwxr-xr-x  8 root root 4096 Dec  8 23:36 adminapi
drwxr-xr-x  2 root root 4096 Dec  8 22:53 bin
drwxr-xr-x  5 root root 4096 Dec  8 22:53 build
drwxr-xr-x  3 root root 4096 Dec  8 22:53 calyptos
drwxr-xr-x  2 root root 4096 Dec  8 22:53 calyptos.egg-info
-rw-r--r--  1 root root 3309 Dec  8 22:53 calyptos.spec
drwxr-xr-x  7 root root 4096 Dec  8 23:14 chef-repo
-rw-r--r--  1 root root  231 Dec  8 22:53 config_data
-rwxr-xr-x  1 root root 1763 Dec  8 23:50 create_nephoria_env.sh
drwxr-xr-x  2 root root 4096 Dec  8 22:41 deploy-helpers
drwxr-xr-x  2 root root 4096 Dec  8 22:53 dist
drwxr-xr-x  2 root root 4096 Dec  8 22:53 docs
-rw-r--r--  1 root root 4172 Dec  8 22:53 environment.yml
drwxr-xr-x  5 root root 4096 Dec  8 23:36 envnephoria
drwxr-xr-x  2 root root 4096 Dec  8 22:53 etc
drwxr-xr-x  6 root root 4096 Dec  8 22:41 eucalele
drwxr-xr-x 12 root root 4096 Dec  8 22:56 eucalyptus-cookbook
drwxr-xr-x  2 root root 4096 Dec  8 22:53 examples
-rw-r--r--  1 root root  532 Dec  8 22:53 fabfile.py
-rw-r--r--  1 root root  829 Dec  8 22:53 fabfile.pyc
-rw-r--r--  1 root root  696 Dec  8 22:53 get_hosts.py
drwxr-xr-x  8 root root 4096 Dec  8 22:53 .git
-rw-r--r--  1 root root  166 Dec  8 22:53 .gitignore
-rw-r--r--  1 root root 1348 Dec  8 22:53 LICENSE
-rw-r--r--  1 root root  373 Dec  8 22:53 machine_list
-rw-r--r--  1 root root  102 Dec  8 22:53 MANIFEST.in
drwxr-xr-x 10 root root 4096 Dec  8 23:39 nephoria
drwxr-xr-x  2 root root 4096 Dec  8 22:53 pxe_manager
-rw-r--r--  1 root root 2588 Dec  8 22:53 README.md
drwxr-xr-x  2 root root 4096 Dec  8 22:53 resource_manager
-rw-r--r--  1 root root 6457 Dec  8 22:53 setup.py
drwxr-xr-x  2 root root 4096 Dec  8 22:53 tests
total 104
drwxr-xr-x 10 root root  4096 Dec  8 23:39 .
drwxr-xr-x 21 root root  4096 Dec  8 23:37 ..
drwxr-xr-x  4 root root  4096 Dec  8 23:37 build
drwxr-xr-x  2 root root  4096 Dec  8 23:37 dist
drwxr-xr-x  3 root root  4096 Dec  8 23:37 docs
drwxr-xr-x  8 root root  4096 Dec  8 23:43 .git
-rw-r--r--  1 root root   115 Dec  8 23:37 .gitignore
-rw-r--r--  1 root root  1427 Dec  8 23:37 LICENSE
-rw-------  1 root root  1769 Dec  8 23:39 LoadBfebsImage_1481240344.pem
drwxr-xr-x  7 root root  4096 Dec  8 23:37 nephoria
drwxr-xr-x  2 root root  4096 Dec  8 23:37 nephoria.egg-info
-rw-r--r--  1 root root 31622 Dec  8 23:43 nephoria-install.log
drwxr-xr-x  2 root root  4096 Dec  8 23:37 nephoria_unit_tests
-rw-r--r--  1 root root 10275 Dec  8 23:37 README.md
-rw-r--r--  1 root root  2826 Dec  8 23:37 setup.py
drwxr-xr-x  2 root root  4096 Dec  8 23:37 toolbox
-rw-r--r--  1 root root   333 Dec  8 23:37 .travis.yml
Fetching origin
origin	git@git.qa1.eucalyptus-systems.com:qa-repos/nephoria.git (fetch)
origin	git@git.qa1.eucalyptus-systems.com:qa-repos/nephoria.git (push)
* master
  remotes/origin/HEAD -> origin/master
  remotes/origin/alice-console
  remotes/origin/batchtest
  remotes/origin/bfebs
  remotes/origin/getimage
  remotes/origin/imageurl
  remotes/origin/ins_test_fix
  remotes/origin/installscript
  remotes/origin/master
  remotes/origin/oldboto
  remotes/origin/sosips
Already on 'master'
From git.qa1.eucalyptus-systems.com:qa-repos/nephoria
 * branch            master     -> FETCH_HEAD
Already up-to-date.
/mnt/swathi/dev-swathi-EUCA-12910/97
+ cd nephoria
+ cli_args='  '
+ [[ '' != '' ]]
+ cli_args='   --environment-file ../environment.yml '
+ export EUTESTER_FORCE_ANSI_ESCAPE=TRUE
+ EUTESTER_FORCE_ANSI_ESCAPE=TRUE
+ ../envnephoria/bin/python nephoria/testcases/ec2/ebs/legacy_ebs_test_suite.py --environment-file ../environment.yml --password foobar
[12-08 23:50:57][INFO][LegacyEbsTestSuite]:
               TEST CASE INFO
 +------------------+--------------------+
 | NAME             | LegacyEbsTestSuite |
 | TEST LIST        | []                 |
 | ENVIRONMENT FILE | ../environment.yml |
 +------------------+--------------------+

[12-08 23:50:57][INFO][LegacyEbsTestSuite]:
+-------------------+----------------------------------+
| TEST ARGS         | VALUE                            |
+-------------------+----------------------------------+
| access_key        | None                             |
| clc               | None                             |
| clean_on_exit     | True                             |
| configsections    | ['LegacyEbsTestSuite', 'global'] |
| dry_run           | False                            |
| emi               | None                             |
| environment_file  | ../environment.yml               |
| exit_on_failure   | False                            |
| group             | None                             |
| instance_password | None                             |
| log_file          | None                             |
| log_file_level    | DEBUG                            |
| log_level         | DEBUG                            |
| no_clean          | False                            |
| password          | foobar                           |
| region            | None                             |
| root_device_type  | instance-store                   |
| secret_key        | None                             |
| test_account      | testrunner                       |
| test_list         | None                             |
| test_regex        | None                             |
| test_user         | admin                            |
| vmtype            | c1.medium                        |
| volumes           | None                             |
| wait_on_progress  | 20                               |
| waitconnect       | 30                               |
| zone              | None                             |
+-------------------+----------------------------------+

[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "create_vols_per_zone" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:create_vols_per_zone, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'volsperzone', 'size', 'snapshot', 'timepergig')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "expand_volume_size" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:expand_volume_size, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'volsperzone', 'size')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "create_test_instances_for_zones" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:create_test_instances_for_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'image', 'keypair', 'username', 'instance_password', 'group', 'vmtype', 'count')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Found matching arg for:group
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Found matching arg for:instance_password
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Found matching arg for:vmtype
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "attach_all_avail_vols_to_instances_in_zones" with args:overwrite = True
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:attach_all_avail_vols_to_instances_in_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{'overwrite': True}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout', 'overwrite')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{'overwrite': True}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "negative_delete_attached_volumes_in_zones" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:negative_delete_attached_volumes_in_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "negative_attach_in_use_volume_in_zones" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:negative_attach_in_use_volume_in_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "create_vols_per_zone" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:create_vols_per_zone, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'volsperzone', 'size', 'snapshot', 'timepergig')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "attach_all_avail_vols_to_instances_in_zones" with args:overwrite = True
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:attach_all_avail_vols_to_instances_in_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{'overwrite': True}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout', 'overwrite')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{'overwrite': True}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "reboot_instances_in_zone_verify_volumes" with args:waitconnect = 30
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:reboot_instances_in_zone_verify_volumes, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{'waitconnect': 30}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'waitconnect', 'timeout')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{'waitconnect': 30}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Found matching arg for:waitconnect
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Skipping populate because testunit already has this arg:waitconnect
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "detach_volumes_in_zones" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:detach_volumes_in_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout', 'volcount', 'eof')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "create_snapshots_all_vols_in_zone" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:create_snapshots_all_vols_in_zone, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'volstate', 'wait_on_progress')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Found matching arg for:wait_on_progress
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "create_vols_from_snap_in_same_zone" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:create_vols_from_snap_in_same_zone, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timepergig')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "attach_new_vols_from_snap_verify_md5" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:attach_new_vols_from_snap_verify_md5, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout', 'timepergig')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "detach_all_volumes_from_stopped_instances_in_zones" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:detach_all_volumes_from_stopped_instances_in_zones, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "terminate_instances_in_zones_verify_volume_detach" with args:
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:terminate_instances_in_zones_verify_volume_detach, with testcase.args...
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: Got method args:('self', 'zonelist', 'timeout')
[12-08 23:50:57][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-08 23:50:57][INFO][create_vols_per_zone]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: create_vols_per_zone                                                                   |
 | METHOD:create_vols_per_zone, TEST DESCRIPTION:                                                            |
 |                                                                                                           |
 | Description:                                                                                              |
 | Intention of this test is to verify creation of volume(s) per zone given.                                 |
 | Upon successful creation the volumes will be appended to a volumes list                                   |
 | for the zone it was created in.                                                                           |
 | These volumes may be later used if in later ebstests suite tests.                                         |
 |                                                                                                           |
 | End on Failure:True                                                                                       |
 | Passing ARGS:""                                                                                           |
 | Running test method: "create_vols_per_zone(timepergig=300, snapshot=None, volsperzone=1, zonelist=None,   |
 | size=1, )"                                                                                                |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

AUTOCREDS DOMAIN:None REGION:None, SC:None
[12-08 23:50:57][INFO][SystemConnection]: Updating with region:None and domainNone
[12-08 23:50:57][DEBUG][SystemConnection]: Can't create serviceconnection without clc_ip, access, and secret key
[12-08 23:50:57][WARNING][SystemConnection]: Cloud domain is not set, this may cause connection problems
[12-08 23:50:57][INFO][SystemConnection]: Done. Updated region to:/ domain to:/
[12-08 23:50:57][DEBUG][SystemConnection]: Trying creds from provided key and region info...
[12-08 23:50:57][DEBUG][SystemConnection]: Trying creds from local file...
[12-08 23:50:57][DEBUG][SystemConnection]: Trying creds from service connection...
[12-08 23:50:57][DEBUG][SystemConnection]: Trying creds from assume admin role on clc...
[12-08 23:50:57][DEBUG][10.111.5.112]: test_port_status, ip:10.111.5.112, port:22, TCP:True
[12-08 23:50:57][DEBUG][10.111.5.112]: test_port_status, success
[12-08 23:50:57][DEBUG][10.111.5.112]: SSH connection has hostname:10.111.5.112 user:root password:f****r
[12-08 23:50:57][DEBUG][10.111.5.112]: SSH connection attempt(1 of 3), host:'root@10.111.5.112', using ipv4:10.111.5.112, thru proxy:'None'
[12-08 23:50:57][DEBUG][10.111.5.112]: SSH - Connected to 10.111.5.112
[12-08 23:50:57][DEBUG][10.111.5.112]: [root@10.111.5.112]# clcadmin-assume-system-credentials
[12-08 23:50:57][DEBUG][10.111.5.112]:
export AWS_IAM_URL=http://127.0.0.1:8773/services/Euare;
export EUCA_BOOTSTRAP_URL=http://127.0.0.1:8773/services/Empyrean;
export EUCA_PROPERTIES_URL=http://127.0.0.1:8773/services/Properties;
export AWS_ACCESS_KEY_ID="AKIAAHUUPV4A6WHHU7MF";
export AWS_ACCESS_KEY="AKIAAHUUPV4A6WHHU7MF";
export EC2_ACCESS_KEY="AKIAAHUUPV4A6WHHU7MF";
export AWS_SECRET_ACCESS_KEY="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";
export AWS_SECRET_KEY="Q2uVo27Oc6avA0cKZQXApHjFf01UJ9PB697pqKWP";
export EC2_SECRET_KEY="Q2uVo27Oc6avA0cKZQXApHjFf01UJ9PB697pqKWP";
unset AWS_CREDENTIAL_FILE;
unset AWS_SESSION_TOKEN;
unset AWS_SECURITY_TOKEN;
unset EC2_USER_ID;

# These are automatic CLC admin credentials generated on
# Thu Dec  8 23:50:57 UTC 2016.
#
# If you can read this, rerun this program with eval:
#     eval `/usr/sbin/clcadmin-assume-system-credentials`

[12-08 23:50:57][DEBUG][10.111.5.112]: done with exec
[12-08 23:50:57][DEBUG][SystemConnection]: Trying creds from service connection...
[12-08 23:50:57][DEBUG][SystemConnection]: Derived creds from provided keys and region info
[12-08 23:50:58][DEBUG][TESTER:10.111.5.112]: Attempting to create user with params: account:testrunner, name:adminaccess_key:None, secret_key:None, credpath:None, eucarc:None, machine:None, service_connection:None, path:/, region:None,loglevel:10, https:False, boto2_api_version:2015-10-01
[12-08 23:50:58][DEBUG][SystemConnection]: Setting domain to Value:g-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:50:58][INFO][SystemConnection]: Updating with region:None and domaing-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:50:58][INFO][SystemConnection]: Done. Updated region to:/ domain to:g-05-03.autoqa.qa1.eucalyptus-systems.com/g-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:50:58][DEBUG][SystemConnection]: Setting region to Value:None
[12-08 23:50:58][INFO][SystemConnection]: Updating with region:None and domainNone
[12-08 23:50:58][INFO][SystemConnection]: Done. Updated region to:/ domain to:g-05-03.autoqa.qa1.eucalyptus-systems.com/g-05-03.autoqa.qa1.eucalyptus-systems.com
AUTOCREDS DOMAIN:g-05-03.autoqa.qa1.eucalyptus-systems.com REGION:None, SC:10.111.5.112:SystemConnection
[12-08 23:50:58][INFO][UserContext]: Updating with region:None and domainNone
[12-08 23:50:58][INFO][UserContext]: Done. Updated region to:/ domain to:g-05-03.autoqa.qa1.eucalyptus-systems.com/g-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:50:58][DEBUG][IAMops(admin:eucalyptus)]: Creating ops: IAMops
[12-08 23:50:58][DEBUG][IAMops(admin:eucalyptus)]: get region info params: host:iam.g-05-03.autoqa.qa1.eucalyptus-systems.com, endpoint:iam.g-05-03.autoqa.qa1.eucalyptus-systems.com, region_name:None
[12-08 23:50:58][DEBUG][IAMops(admin:eucalyptus)]: Attempting to fetch all accounts matching- account_id:None account_name:eucalyptus
[12-08 23:50:58][DEBUG][UserContext:000501514173::eucalyptus::admin]: Successfully created User Context in Region:, Domain:g-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:50:58][DEBUG][IAMops(admin:eucalyptus)]: Attempting to fetch all accounts matching- account_id:None account_name:testrunner
[12-08 23:50:58][DEBUG][IAMops(admin:eucalyptus)]: create_account(). Account already exists: testrunner
[12-08 23:50:58][DEBUG][IAMops(admin:eucalyptus)]: create_user(). User already exists: admin
[12-08 23:50:59][WARNING][UserContext:000501514173::eucalyptus::admin]: Failed to fetch vmware brokers, vmware may not be supported on this cloud. Err:Failed to find corresponding class mapping for element: DescribeVMwareBrokers in namespace: msgs_eucalyptus_com_4_4_0
AUTOCREDS DOMAIN:g-05-03.autoqa.qa1.eucalyptus-systems.com REGION:None, SC:10.111.5.112:SystemConnection
[12-08 23:50:59][INFO][UserContext]: Updating with region:None and domainNone
[12-08 23:50:59][INFO][UserContext]: Done. Updated region to:/ domain to:g-05-03.autoqa.qa1.eucalyptus-systems.com/g-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:50:59][DEBUG][UserContext:::testrunner::admin]: Trying creds from provided key and region info...
[12-08 23:50:59][DEBUG][UserContext:::testrunner::admin]: Trying creds from local file...
[12-08 23:50:59][DEBUG][UserContext:::testrunner::admin]: Trying creds from service connection...
[12-08 23:50:59][DEBUG][UserContext:::testrunner::admin]: Trying creds from remote machine at credpath...
[12-08 23:50:59][DEBUG][UserContext:::testrunner::admin]: Trying creds from CLC DB...
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: test_port_status, ip:10.111.5.112, port:22, TCP:True
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: test_port_status, success
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: SSH connection has hostname:10.111.5.112 user:root password:f****r
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: SSH connection attempt(1 of 3), host:'root@10.111.5.112', using ipv4:10.111.5.112, thru proxy:'None'
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: SSH - Connected to 10.111.5.112
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: [root@10.111.5.112]# env | grep EUCALYPTUS
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]:

[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: done with exec
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: [root@10.111.5.112]# [ -d /opt/eucalyptus ]
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]:

[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: done with exec
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: []
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: [root@10.111.5.112]# ls /usr/sbin/ | grep eucalyptus
[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]:
eucalyptus-cloud
eucalyptus-cluster
eucalyptus-node

[12-08 23:50:59][DEBUG][10.111.5.112:(['CLC', 'SC', 'CC', 'WS'])]: done with exec
[12-08 23:50:59][DEBUG][UserContext:::testrunner::admin]: Attempting to fetch port from property:"bootstrap.webservices.port"
[12-08 23:51:00][DEBUG][UserContext:000506486350::testrunner::admin]: Successfully created User Context in Region:, Domain:g-05-03.autoqa.qa1.eucalyptus-systems.com
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: Creating ops: EC2ops
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: get region info params: host:ec2.g-05-03.autoqa.qa1.eucalyptus-systems.com, endpoint:ec2.g-05-03.autoqa.qa1.eucalyptus-systems.com, region_name:None
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1916)Starting method: create_volumes(self, zone=one, size=1, count=1, mincount=None, eof=True, monitor_to_state=available, delay=0, snapshot=None, timeout=0, poll_interval=10, timepergig=300)
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: Sending create volume request, count:1
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: 1/1 requests for volume creation succeeded.
[12-08 23:51:00][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+----+----+--------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS|AGE |SIZE|SRC_SNAP| MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+----+----+--------+-----------+----+--------+
|vol-0147336f|  0  | creating |0.12| 1  |  None  |None/(1024)|one |  None  |
+------------+-----+----------+----+----+--------+-----------+----+--------+

[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2015)Starting method: monitor_created_euvolumes_to_state(self, volumes=[Volume:vol-0147336f], eof=True, mincount=1, state=available, poll_interval=10, deletefailed=True, size=1, timepergig=300)
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: Polling 1 volumes for status:"available"...
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-0147336f) State(creating), seconds elapsed: 0/300
[12-08 23:51:00][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:0, Waiting on 1 volumes to enter state:available-----
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-0147336f) State(available), seconds elapsed: 10/300
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:10, Waiting on 0 volumes to enter state:available-----
[12-08 23:51:10][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+-----+----+--------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS| AGE |SIZE|SRC_SNAP| MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+-----+----+--------+-----------+----+--------+
|vol-0147336f|  0  |available |10.21| 1  |  None  |None/(1024)|one |  None  |
+------------+-----+----------+-----+----+--------+-----------+----+--------+

[12-08 23:51:10][DEBUG][create_vols_per_zone]: create_vols_per_zone created vols(1) zone:one
[12-08 23:51:10][INFO][create_vols_per_zone]:
----------------------------------------------------------------------------------
                - SUCCESS -  TEST:"create_vols_per_zone" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:51:10][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        1         14        13
-----------------------------------------------

[12-08 23:51:10][INFO][expand_volume_size]:
---------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------+
 | STARTING TESTUNIT: expand_volume_size                                             |
 | METHOD:expand_volume_size, TEST DESCRIPTION:                                      |
 |                                                                                   |
 | Description:                                                                      |
 | Intention of this test is to verify creation of volume(s) from a                  |
 | snapshot and expanding the size of the volume                                     |
 |                                                                                   |
 | End on Failure:True                                                               |
 | Passing ARGS:""                                                                   |
 | Running test method: "expand_volume_size(volsperzone=1, zonelist=None, size=1, )" |
 +-----------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------

[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1916)Starting method: create_volumes(self, zone=one, size=1, count=1, mincount=None, eof=True, monitor_to_state=available, delay=0, snapshot=None, timeout=0, poll_interval=10, timepergig=120)
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: Sending create volume request, count:1
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: 1/1 requests for volume creation succeeded.
[12-08 23:51:10][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+----+----+--------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS|AGE |SIZE|SRC_SNAP| MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+----+----+--------+-----------+----+--------+
|vol-0b3c3228|  0  | creating |0.08| 1  |  None  |None/(1024)|one |  None  |
+------------+-----+----------+----+----+--------+-----------+----+--------+

[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2015)Starting method: monitor_created_euvolumes_to_state(self, volumes=[Volume:vol-0b3c3228], eof=True, mincount=1, state=available, poll_interval=10, deletefailed=True, size=1, timepergig=120)
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: Polling 1 volumes for status:"available"...
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-0b3c3228) State(creating), seconds elapsed: 0/120
[12-08 23:51:10][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:0, Waiting on 1 volumes to enter state:available-----
[12-08 23:51:20][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-0b3c3228) State(available), seconds elapsed: 10/120
[12-08 23:51:20][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:10, Waiting on 0 volumes to enter state:available-----
[12-08 23:51:20][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+-----+----+--------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS| AGE |SIZE|SRC_SNAP| MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+-----+----+--------+-----------+----+--------+
|vol-0b3c3228|  0  |available |10.16| 1  |  None  |None/(1024)|one |  None  |
+------------+-----+----------+-----+----+--------+-----------+----+--------+

[12-08 23:51:20][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2646)Starting method: create_snapshot_from_volume(self, volume=Volume:vol-0b3c3228, wait_on_progress=40, poll_interval=10, timeout=0, description=)
[12-08 23:51:20][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2730)Starting method: create_snapshots(self, volume=Volume:vol-0b3c3228, count=1, mincount=1, eof=True, delay=0, wait_on_progress=40, poll_count=48, poll_interval=10, timeout=0, monitor_to_completed=True, delete_failed=True, description=)
[12-08 23:51:20][DEBUG][EC2ops(admin:testrunner)]: Create_snapshots count:1, mincount:1, wait_on_progress:40,eof:True
[12-08 23:51:21][DEBUG][EC2ops(admin:testrunner)]: Attempting to create snapshot #0, id:snap-92bbc7b1
[12-08 23:51:21][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2854)Starting method: monitor_eusnaps_to_completed(self, snaps=[Snapshot:snap-92bbc7b1], mincount=1, eof=True, wait_on_progress=40, poll_count=48, poll_interval=10, timeout=0, monitor_to_progress=None, delete_failed=True)
[12-08 23:51:21][DEBUG][EC2ops(admin:testrunner)]: Monitor_snapshot_to_completed starting...
[12-08 23:51:21][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to go to completed state...
[12-08 23:51:21][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-08 23:51:21][DEBUG][EC2ops(admin:testrunner)]: snap-92bbc7b1, Status:pending, Progress:0%, Polls w/o progress:1/40, Time Elapsed:0/0
[12-08 23:51:31][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-08 23:51:31][DEBUG][EC2ops(admin:testrunner)]: snap-92bbc7b1, Status:pending, Progress:25%, Polls w/o progress:0/40, Time Elapsed:10/0
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: snap-92bbc7b1, Status:completed, Progress:100%, Polls w/o progress:0/40, Time Elapsed:20/0
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: snap-92bbc7b1 created after 20 seconds. Status:completed, Progress:100%
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3044)Starting method: delete_snapshots(self, snapshots=[], valid_states=completed,failed, base_timeout=60, add_time_per_snap=10, wait_for_valid_state=120, poll_interval=10, eof=False)
[12-08 23:51:41][INFO][EC2ops(admin:testrunner)]:
+---------------+-------+---------+---------+-----+-----------+--------------------+---------------+----------+
|    SNAP_ID    | ORDER | CMDTIME | ELAPSED |  %  |   STATUS  |   SRC_VOL:(ZONE)   | SRC_MD5:(LEN) | INFO_MSG |
+---------------+-------+---------+---------+-----+-----------+--------------------+---------------+----------+
| snap-92bbc7b1 |   0   |   0.74  |    20   | 100 | completed | vol-0b3c3228:(one) |  None:(1024)  | SUCCESS  |
+---------------+-------+---------+---------+-----+-----------+--------------------+---------------+----------+

[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1895)Starting method: create_volume(self, zone=one, size=2, eof=True, snapshot=Snapshot:snap-92bbc7b1, timeout=0, poll_interval=10, timepergig=120)
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1916)Starting method: create_volumes(self, zone=one, size=2, count=1, mincount=1, eof=True, monitor_to_state=available, delay=0, snapshot=Snapshot:snap-92bbc7b1, timeout=0, poll_interval=10, timepergig=120)
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: Sending create volume request, count:1
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: 1/1 requests for volume creation succeeded.
[12-08 23:51:41][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+----+----+-------------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS|AGE |SIZE|   SRC_SNAP  | MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+----+----+-------------+-----------+----+--------+
|vol-00fe4786|  0  | creating |0.10| 2  |snap-92bbc7b1|None/(1024)|one |  None  |
+------------+-----+----------+----+----+-------------+-----------+----+--------+

[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2015)Starting method: monitor_created_euvolumes_to_state(self, volumes=[Volume:vol-00fe4786], eof=True, mincount=1, state=available, poll_interval=10, deletefailed=True, size=1, timepergig=120)
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: Polling 1 volumes for status:"available"...
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-00fe4786) State(creating), seconds elapsed: 0/240
[12-08 23:51:41][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:0, Waiting on 1 volumes to enter state:available-----
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-00fe4786) State(available), seconds elapsed: 10/240
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:10, Waiting on 0 volumes to enter state:available-----
[12-08 23:51:51][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+-----+----+-------------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS| AGE |SIZE|   SRC_SNAP  | MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+-----+----+-------------+-----------+----+--------+
|vol-00fe4786|  0  |available |10.18| 2  |snap-92bbc7b1|None/(1024)|one |  None  |
+------------+-----+----------+-----+----+-------------+-----------+----+--------+

[12-08 23:51:51][INFO][expand_volume_size]:
----------------------------------------------------------------------------------
                 - SUCCESS -  TEST:"expand_volume_size" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:51:51][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        2         13        54
-----------------------------------------------

[12-08 23:51:51][INFO][create_test_instances_for_zones]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: create_test_instances_for_zones                                                        |
 | METHOD:create_test_instances_for_zones, TEST DESCRIPTION:                                                 |
 |                                                                                                           |
 | Description:                                                                                              |
 | Create an instance within each TestZone object in zonelist to help test                                   |
 | ebs functionality.                                                                                        |
 |                                                                                                           |
 | End on Failure:True                                                                                       |
 | Passing ARGS:                                                                                             |
 | ---------------------                                                                                     |
 | vmtype : c1.medium                                                                                        |
 | group : None                                                                                              |
 | instance_password : None                                                                                  |
 | ---------------------                                                                                     |
 | Running test method: "create_test_instances_for_zones(username=root, count=1, group=None, image=None,     |
 | instance_password=None, vmtype=c1.medium, keypair=None, zonelist=None, )"                                 |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

KWARG:vmtype = c1.medium
KWARG:group = None
KWARG:instance_password = None
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3481)Starting method: get_emi(self, emi=None, name=None, root_device_type=None, root_device_name=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=None, platform=None, not_platform=None, tagkey=None, tagvalue=None, _args_dict=None, virtualization_type=None)
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=None, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters={'image-type': 'machine', 'tag-key': 'nephoria-created'}, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': None, 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': {'image-type': 'machine', 'tag-key': 'nephoria-created'}, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'image-type': 'machine', 'tag-key': 'nephoria-created'}"
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Got 0 total images None, now filtering...
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=None, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters={'image-type': 'machine'}, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': None, 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': {'image-type': 'machine'}, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'image-type': 'machine'}"
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Got 2 total images None, now filtering...
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Returning image:emi-08c9df37
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:608)Starting method: add_group(self, group_name=LegacyEbsTestSuite_group, description=None, vpc_id=None, fail_if_exists=False)
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Looking up group LegacyEbsTestSuite_group
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Creating Security Group: LegacyEbsTestSuite_group
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:683)Starting method: authorize_group(self, group=SecurityGroup:LegacyEbsTestSuite_group, group_name=None, group_id=None, port=22, end_port=None, protocol=tcp, cidr_ip=0.0.0.0/0, src_security_group=None, src_security_group_id=None, src_security_group_name=None, src_security_group_owner_id=None)
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:683)Starting method: authorize_group(self, group=SecurityGroup:LegacyEbsTestSuite_group, group_name=None, group_id=None, port=-1, end_port=None, protocol=icmp, cidr_ip=0.0.0.0/0, src_security_group=None, src_security_group_id=None, src_security_group_name=None, src_security_group_owner_id=None)
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Found matching security group for name:None and id:sg-160f02c0
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Found matching security group for name:None and id:sg-160f02c0
[12-08 23:51:51][INFO][EC2ops(admin:testrunner)]:
 Security Group: LegacyEbsTestSuite_group/sg-160f02c0, VPC: None
 INGRESS RULES:
     +---------------------+--------------------------+-------------+--------------+-------+----------+-------+
       CIDR_IP               SRC_GRP_NAME               SRC_GRP_ID    OWNER_ID       PORT    END_PORT   PROTO
     +---------------------+--------------------------+-------------+--------------+-------+----------+-------+
       0.0.0.0/0             None                       None          None           22      22         tcp
       0.0.0.0/0             None                       None          None           -1      -1         icmp
     +---------------------+--------------------------+-------------+--------------+-------+----------+-------+

[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Looking up keypair LegacyEbsTestSuite_1481241111
[12-08 23:51:51][DEBUG][EC2ops(admin:testrunner)]: Creating keypair: LegacyEbsTestSuite_1481241111
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:4036)Starting method: run_image(self, image=Image:emi-08c9df37, keypair=LegacyEbsTestSuite_1481241111, group=SecurityGroup:LegacyEbsTestSuite_group, vmtype=c1.medium, zone=one, min=1, max=1, block_device_map=None, user_data=None, private_addressing=False, username=root, password=None, subnet_id=None, auto_connect=True, clean_on_fail=True, monitor_to_running=True, return_reservation=False, auto_create_eni=True, network_interfaces=None, check_enis=True, timeout=480, systemconnection=None, boto_debug_level=2)
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Euinstance list prior to running image...
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5847)Starting method: get_instances(self, idstring=None, state=None, reservation=None, rootdevtype=None, zone=None, key=None, pubip=None, privip=None, ramdisk=None, kernel=None, image_id=None, verbose=None, filters=None)
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3481)Starting method: get_emi(self, emi=emi-18a58624, name=None, root_device_type=None, root_device_name=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=None, platform=None, not_platform=None, tagkey=None, tagvalue=None, _args_dict={'args': (EC2ops::ec2, u'emi-18a58624'), 'kwargs': {'_args_dict': {...}}}, virtualization_type=None)
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=emi-18a58624, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': u'emi-18a58624', 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': None, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'state': 'available', 'image-id': u'emi-18a58624'}"
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Got 1 total images emi-18a58624, now filtering...
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Returning image:emi-18a58624
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3481)Starting method: get_emi(self, emi=emi-18a58624, name=None, root_device_type=None, root_device_name=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=None, platform=None, not_platform=None, tagkey=None, tagvalue=None, _args_dict={'args': (EC2ops::ec2, u'emi-18a58624'), 'kwargs': {'_args_dict': {...}}}, virtualization_type=None)
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=emi-18a58624, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': u'emi-18a58624', 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': None, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'state': 'available', 'image-id': u'emi-18a58624'}"
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Got 1 total images emi-18a58624, now filtering...
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Returning image:emi-18a58624
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3481)Starting method: get_emi(self, emi=emi-18a58624, name=None, root_device_type=None, root_device_name=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=None, platform=None, not_platform=None, tagkey=None, tagvalue=None, _args_dict={'args': (EC2ops::ec2, u'emi-18a58624'), 'kwargs': {'_args_dict': {...}}}, virtualization_type=None)
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=emi-18a58624, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': u'emi-18a58624', 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': None, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'state': 'available', 'image-id': u'emi-18a58624'}"
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Got 1 total images emi-18a58624, now filtering...
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Returning image:emi-18a58624
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:
Euinstance list prior to running image:
+---------------------+--------------------+----------------------+-----------------------------------------------------------+
|INSTANCE ID          |INSTANCE IMAGE      |INSTANCE STATE        |INSTANCE NETWORK INFO:                                     |
+---------------------+--------------------+----------------------+-----------------------------------------------------------+
|ID: i-370d892a       |EMI: emi-18a58624   |STATE: terminated     |NODE:???              KEYPAIR:LoadBfebsImage_1481240344    |
|TYPE: m1.small       |OS: linux           |AGE: 123              |+----+------+-----------+-+---------------+---------------+|
|RES: r-841daa25      |VIRT: HVM           |ZONE: one             ||VPC |SUBNET|  SEC GRPS |P|    PRIV IP    |     PUB IP    ||
|ACCOUNT ID:          |IMAGE NAME:         |ROOTDEV:              |+----+------+-----------+-+---------------+---------------+|
|000506486350         |preciseservercloud..|instance-store        ||None| None |           |N|      None     |      None     ||
+---------------------+--------------------+----------------------+-----------------------------------------------------------+
|ID: i-6d45ce13       |EMI: emi-18a58624   |STATE: terminated     |NODE:???              KEYPAIR:LoadBfebsImage_1481240344    |
|TYPE: m1.small       |OS: linux           |AGE: 505              |+----+------+-----------+-+---------------+---------------+|
|RES: r-23f36e6d      |VIRT: HVM           |ZONE: one             ||VPC |SUBNET|  SEC GRPS |P|    PRIV IP    |     PUB IP    ||
|ACCOUNT ID:          |IMAGE NAME:         |ROOTDEV:              |+----+------+-----------+-+---------------+---------------+|
|000506486350         |preciseservercloud..|instance-store        ||None| None |           |N|      None     |      None     ||
+---------------------+--------------------+----------------------+-----------------------------------------------------------+
|ID: i-5e3cf8c0       |EMI: emi-18a58624   |STATE: terminated     |NODE:???              KEYPAIR:LoadBfebsImage_1481240344    |
|TYPE: m1.small       |OS: linux           |AGE: 447              |+----+------+-----------+-+---------------+---------------+|
|RES: r-dcf2a7a0      |VIRT: HVM           |ZONE: one             ||VPC |SUBNET|  SEC GRPS |P|    PRIV IP    |     PUB IP    ||
|ACCOUNT ID:          |IMAGE NAME:         |ROOTDEV:              |+----+------+-----------+-+---------------+---------------+|
|000506486350         |preciseservercloud..|instance-store        ||None| None |           |N|      None     |      None     ||
+---------------------+--------------------+----------------------+-----------------------------------------------------------+
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]:

 Making Run instance request....
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: wait_for_instance_block_dev_mapping started...
[12-08 23:51:52][DEBUG][EC2ops(admin:testrunner)]: Waiting for instance block device mapping to be populated:i-da5b9630
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: Instance block device mapping is populated:i-da5b9630
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-94c44f8a, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: wait_for_instance_block_dev_mapping done. elapsed:1.02274608612
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: i-da5b9630:Converting instance to euinstance type.
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: Looking up keypair LegacyEbsTestSuite_1481241111
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:51:53][INFO][EC2ops(admin:testrunner)]: Existing EC2 key LegacyEbsTestSuite_1481241111 and local cert found at: "./LegacyEbsTestSuite_1481241111.pem"
[12-08 23:51:53][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-94c44f8a, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5083)Starting method: monitor_euinstances_to_running(self, instances=[Instance:i-da5b9630], poll_interval=10, timeout=480)
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]: (1) Monitor_instances_to_running starting...
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5422)Starting method: monitor_euinstances_to_state(self, instance_list=[Instance:i-da5b9630], state=running, min=None, poll_interval=10, failstates=['stopped', 'terminated', 'shutting-down'], timeout=480, eof=True)
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]: (1) monitor_instances_to_state: 'running' starting....
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]:
------>Waiting for remaining 1/1 instances to go to state:running, elapsed:(0/480)...
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-94c44f8a, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-08 23:51:54][DEBUG][EC2ops(admin:testrunner)]: WAITING for Intended state:running: i-da5b9630 Current state:pending, type:ebs, backing volume:vol-94c44f8a status:in-use, elapsed:0/480
[12-08 23:52:04][DEBUG][EC2ops(admin:testrunner)]:
------>Waiting for remaining 1/1 instances to go to state:running, elapsed:(10/480)...
[12-08 23:52:04][DEBUG][EC2ops(admin:testrunner)]: WAITING for Intended state:running: i-da5b9630 Current state:pending, type:ebs, backing volume:vol-94c44f8a status:in-use, elapsed:10/480
[12-08 23:52:14][DEBUG][EC2ops(admin:testrunner)]:
------>Waiting for remaining 1/1 instances to go to state:running, elapsed:(20/480)...
[12-08 23:52:14][DEBUG][EC2ops(admin:testrunner)]: WAITING for Intended state:running: i-da5b9630 Current state:pending, type:ebs, backing volume:vol-94c44f8a status:in-use, elapsed:20/480
[12-08 23:52:24][DEBUG][EC2ops(admin:testrunner)]:
------>Waiting for remaining 1/1 instances to go to state:running, elapsed:(30/480)...
[12-08 23:52:24][DEBUG][EC2ops(admin:testrunner)]: WAITING for Intended state:running: i-da5b9630 Current state:pending, type:ebs, backing volume:vol-94c44f8a status:in-use, elapsed:30/480
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
------>Waiting for remaining 1/1 instances to go to state:running, elapsed:(40/480)...
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: SUCCESS Intended state:running: i-da5b9630 Current state:running, type:ebs, backing volume:vol-94c44f8a status:in-use, elapsed:40/480
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: WAITING for Intended state:running: i-da5b9630 Current state:running, type:ebs, backing volume:vol-94c44f8a status:in-use, elapsed:40/480
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3481)Starting method: get_emi(self, emi=emi-08c9df37, name=None, root_device_type=None, root_device_name=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=None, platform=None, not_platform=None, tagkey=None, tagvalue=None, _args_dict={'args': (EC2ops::ec2, u'emi-08c9df37'), 'kwargs': {'_args_dict': {...}}}, virtualization_type=None)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=emi-08c9df37, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': u'emi-08c9df37', 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': None, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'state': 'available', 'image-id': u'emi-08c9df37'}"
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Got 1 total images emi-08c9df37, now filtering...
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Returning image:emi-08c9df37
[12-08 23:52:34][INFO][EC2ops(admin:testrunner)]:
+---------------------+--------------------+-------------------+-----------------------------------------------------------+
|INSTANCE ID          |INSTANCE IMAGE      |INSTANCE STATE     |INSTANCE NETWORK INFO:                                     |
+---------------------+--------------------+-------------------+-----------------------------------------------------------+
|ID: i-da5b9630       |EMI: emi-08c9df37   |STATE: running     |NODE:???              KEYPAIR:LegacyEbsTestSuite_1481241111|
|TYPE: c1.medium      |OS: linux           |AGE: 42            |+----+------+-----------+-+---------------+---------------+|
|RES: r-f4552acd      |VIRT: HVM           |ZONE: one          ||VPC |SUBNET|  SEC GRPS |P|    PRIV IP    |     PUB IP    ||
|ACCOUNT ID:          |IMAGE NAME:         |ROOTDEV:           |+----+------+-----------+-+---------------+---------------+|
|000506486350         |bfebs_snap-716ff5e..|ebs:vol-94c44f8a   ||None| None |sg-160f02c0|N| 10.111.31.130 | 10.111.31.105 ||
+---------------------+--------------------+-------------------+-----------------------------------------------------------+

[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5539)Starting method: wait_for_valid_ip(self, instances=[Instance:i-da5b9630], regex=0.0.0.0, poll_interval=10, timeout=480)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: i-da5b9630: FOUND public ip. Current:10.111.31.105, elapsed:0/480
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Check_system_for_dup_ip starting...
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking reservation: r-841daa25
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking instance i-370d892a          , state:terminated           pubip:None                 privip:None
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking reservation: r-f4552acd
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking instance i-da5b9630          , state:running              pubip:10.111.31.105        privip:10.111.31.130
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking reservation: r-23f36e6d
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking instance i-6d45ce13          , state:terminated           pubip:None                 privip:None
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking reservation: r-dcf2a7a0
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking instance i-5e3cf8c0          , state:terminated           pubip:None                 privip:None
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Done with check_system_for_dup_ip
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Wait_for_valid_ip done
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Instances in running state and wait_for_valid_ip complete, attempting connections...
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking 1 instance ssh connections...
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking instance:i-da5b9630 ...
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5192)Starting method: does_instance_sec_group_allow(self, instance=Instance:i-da5b9630, src_addr=None, src_group=None, protocol=icmp, port=0)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Using src_addr:10.111.54.170
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5310)Starting method: does_sec_group_allow(self, group=SecurityGroup:LegacyEbsTestSuite_group, src_addr=10.111.54.170, src_group=None, protocol=icmp, port=0)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Security group:LegacyEbsTestSuite_group, src ip:10.111.54.170, src_group:None, proto:icmp, port:0
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: rule#1: ports:-1--1, grants:0.0.0.0/0,
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: sec_group DOES allow: group:"LegacyEbsTestSuite_group", src:"10.111.54.170", proto:"icmp", port:"0"
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Sec allows from test source addr: 10.111.54.170, src_group:None, protocol:icmp, port:0
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Do Security group rules allow ping from this test machine:True
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5192)Starting method: does_instance_sec_group_allow(self, instance=Instance:i-da5b9630, src_addr=None, src_group=None, protocol=tcp, port=22)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Using src_addr:10.111.54.170
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:5310)Starting method: does_sec_group_allow(self, group=SecurityGroup:LegacyEbsTestSuite_group, src_addr=10.111.54.170, src_group=None, protocol=tcp, port=22)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Security group:LegacyEbsTestSuite_group, src ip:10.111.54.170, src_group:None, proto:tcp, port:22
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: rule#0: ports:22-22, grants:0.0.0.0/0,
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: sec_group DOES allow: group:"LegacyEbsTestSuite_group", src:"10.111.54.170", proto:"tcp", port:"22"
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Sec allows from test source addr: 10.111.54.170, src_group:None, protocol:tcp, port:22
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Do Security group rules allow ssh from this test machine:True
[12-08 23:52:34][INFO][i-da5b9630]: Attempting to reconnect_to_instance:i-da5b9630
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:52:34][DEBUG][i-da5b9630]: Found local file for ssh keypair, setting keypath to:/mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][i-da5b9630]: reset_ssh_connection for:i-da5b9630
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:52:34][DEBUG][i-da5b9630]: Found local file for ssh keypair, setting keypath to:/mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][i-da5b9630]: Connecting ssh i-da5b9630
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:52:34][DEBUG][i-da5b9630]: Found local file for ssh keypair, setting keypath to:/mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][i-da5b9630]: SSH connection has hostname:10.111.31.105 user:root and keypath: /mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:52:34][DEBUG][i-da5b9630]: SSH connection attempt(1 of 3), host:'root@10.111.31.105', using ipv4:10.111.31.105, thru proxy:'None'
[12-08 23:52:34][DEBUG][i-da5b9630]: SSH - Connected to 10.111.31.105
[12-08 23:52:34][DEBUG][i-da5b9630]: Try some sys...
[12-08 23:52:34][DEBUG][i-da5b9630]: [root@10.111.31.105]#
[12-08 23:52:40][DEBUG][i-da5b9630]:

[12-08 23:52:40][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:40][INFO][i-da5b9630]: SSH Connection Succeeded
[12-08 23:52:40][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:52:40][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:52:40][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]: Connected to instance:i-da5b9630
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3481)Starting method: get_emi(self, emi=emi-08c9df37, name=None, root_device_type=None, root_device_name=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=None, platform=None, not_platform=None, tagkey=None, tagvalue=None, _args_dict={'args': (EC2ops::ec2, u'emi-08c9df37'), 'kwargs': {'_args_dict': {...}}}, virtualization_type=None)
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3359)Starting method: get_images(self, emi=emi-08c9df37, name=None, root_device_type=None, root_device_name=None, virtualization_type=None, location=None, state=available, arch=None, owner_id=None, filters=None, basic_image=False, platform=None, not_platform=None, tagkey=None, tagvalue=None, max_count=1, _args_dict={'args': (EC2ops::ec2,), 'kwargs': {'_args_dict': {...}, 'tagkey': None, 'root_device_type': None, 'name': None, 'basic_image': False, 'emi': u'emi-08c9df37', 'max_count': 1, 'platform': None, 'state': 'available', 'tagvalue': None, 'location': None, 'filters': None, 'not_platform': None, 'owner_id': None, 'arch': None, 'virtualization_type': None, 'root_device_name': None}})
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]: Using following filters for image request:"{'state': 'available', 'image-id': u'emi-08c9df37'}"
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]: Got 1 total images emi-08c9df37, now filtering...
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]: Returning image:emi-08c9df37
[12-08 23:52:40][INFO][EC2ops(admin:testrunner)]:
+---------------------+--------------------+-------------------+-----------------------------------------------------------+
|INSTANCE ID          |INSTANCE IMAGE      |INSTANCE STATE     |INSTANCE NETWORK INFO:                                     |
+---------------------+--------------------+-------------------+-----------------------------------------------------------+
|ID: i-da5b9630       |EMI: emi-08c9df37   |STATE: running     |NODE:???              KEYPAIR:LegacyEbsTestSuite_1481241111|
|TYPE: c1.medium      |OS: linux           |AGE: 48            |+----+------+-----------+-+---------------+---------------+|
|RES: r-f4552acd      |VIRT: HVM           |ZONE: one          ||VPC |SUBNET|  SEC GRPS |P|    PRIV IP    |     PUB IP    ||
|ACCOUNT ID:          |IMAGE NAME:         |ROOTDEV:           |+----+------+-----------+-+---------------+---------------+|
|000506486350         |bfebs_snap-716ff5e..|ebs:vol-94c44f8a   ||None| None |sg-160f02c0|N| 10.111.31.130 | 10.111.31.105 ||
+---------------------+--------------------+-------------------+-----------------------------------------------------------+

[12-08 23:52:40][DEBUG][create_test_instances_for_zones]: Created instance: i-da5b9630 in zone:one
[12-08 23:52:40][INFO][create_test_instances_for_zones]:
----------------------------------------------------------------------------------
           - SUCCESS -  TEST:"create_test_instances_for_zones" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:52:40][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        3         12       102
-----------------------------------------------

[12-08 23:52:40][INFO][attach_all_avail_vols_to_instances_in_zones]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: attach_all_avail_vols_to_instances_in_zones                                            |
 | METHOD:attach_all_avail_vols_to_instances_in_zones, TEST DESCRIPTION:                                     |
 |                                                                                                           |
 | Description:                                                                                              |
 | Iterates though zones and attempts to attach volumes to an instance                                       |
 | within each zone.                                                                                         |
 |                                                                                                           |
 | :parram zonelist: list of zones to include in test                                                        |
 | :param timeout: timeout used for attach volume method                                                     |
 | :param overwrite: boolean to indicate whether a non-zero filled volume should have                        |
 | new unique data prepended for md5sum.                                                                     |
 | This should be used when zero fill volume property is not in use                                          |
 | upon volume first attach. It should not be used after the 1st attach                                      |
 | and volume has been converted to a euvolume within this test.                                             |
 |                                                                                                           |
 | End on Failure:True                                                                                       |
 | Passing ARGS:                                                                                             |
 | ---------------------                                                                                     |
 | overwrite : True                                                                                          |
 | ---------------------                                                                                     |
 | Running test method: "attach_all_avail_vols_to_instances_in_zones(timeout=480, zonelist=None,             |
 | overwrite=True, )"                                                                                        |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

KWARG:overwrite = True
[12-08 23:52:40][DEBUG][attach_all_avail_vols_to_instances_in_zones]: Attempting to attach to 0/[Instance:i-da5b9630] instances in zone:one
[12-08 23:52:40][DEBUG][i-da5b9630]: Attempting to attach volume:vol-0147336f to instance:i-da5b9630 to dev:None
[12-08 23:52:40][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:52:40][DEBUG][i-da5b9630]:
vda
vda1

[12-08 23:52:40][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:40][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:52:40][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:52:40][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-, status=None, attached_instance=i-da5b9630, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-08 23:52:40][DEBUG][i-da5b9630]: Instance:i-da5b9630 returning available cloud scsi dev:/dev/vde
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2472)Starting method: attach_volume(self, instance=Instance:i-da5b9630, volume=Volume:vol-0147336f, device_path=/dev/vde, pause=10, timeout=480)
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]: Sending attach for Volume:vol-0147336f to be attached to Instance:i-da5b9630 at requested device  /dev/vde
[12-08 23:52:40][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f, state:in-use, attached status:attaching, elapsed:0/480
[12-08 23:52:50][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f, Attached: in-use - attached, elapsed:10
[12-08 23:52:50][DEBUG][i-da5b9630]: Checking for volume attachment on guest, elapsed time(0)
[12-08 23:52:50][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:52:50][DEBUG][i-da5b9630]:
vda
vda1
vdb

[12-08 23:52:50][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:50][DEBUG][i-da5b9630]: dev_list_after:vda vda1 vdb
[12-08 23:52:50][DEBUG][i-da5b9630]: Volume:vol-0147336f guest device:/dev/vdb
[12-08 23:52:50][DEBUG][i-da5b9630]: vol-0147336f Requested dev:/dev/vde, attached to guest device:/dev/vdb
[12-08 23:52:50][DEBUG][i-da5b9630]: vol-0147336fFound attached to guest at dev:/dev/vdb, after elapsed:0
Beginning poll loop for result try_to_write_to_disk to go to True
[12-08 23:52:50][DEBUG][i-da5b9630]:
--->(euinstance.py:1445)Starting method: random_fill_volume(self, euvolume=Volume:vol-0147336f, srcdev=None, length=32, timepergig=90)
[12-08 23:52:50][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:52:50][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:52:50][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:50][DEBUG][i-da5b9630]: exit code:0
[12-08 23:52:50][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:52:50][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/urandom
[12-08 23:52:51][DEBUG][i-da5b9630]:
/dev/urandom

[12-08 23:52:51][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:51][DEBUG][i-da5b9630]:
--->(machine.py:1254)Starting method: dd_monitor(self, ddif=None, ddof=None, ddcount=None, ddbs=1024, ddbytes=None, ddcmd=echo vol-0147336f | dd of=/dev/vdb, ddseek=None, timeout=90, poll_interval=1, tmpfile=None, sync=False)
[12-08 23:52:51][DEBUG][i-da5b9630]: [root@10.111.31.105]# nohup echo vol-0147336f | dd of=/dev/vdb 2> /tmp/eutesterddcmd.10.111.31.105.1171 & echo $! && sleep 2
[12-08 23:52:51][DEBUG][i-da5b9630]:
1104
nohup: ignoring input and redirecting stderr to stdout

[12-08 23:52:53][DEBUG][i-da5b9630]: done with exec

----------------------------------------------------------------------------------------------------------------------------
DD DATA INFO                            |     DD TIME INFO    |       DD RATE INFO      |     DD RECORDS FULL/PARTIAL INFO
----------------------------------------------------------------------------------------------------------------------------
BYTES          |      MBs      |  GIGs  | DD TIME  |TEST TIME |  DD RATE   | TEST RATE  |      REC_IN      |     REC_OUT
----------------------------------------------------------------------------------------------------------------------------

13             |      0.0      |  0.0   |0.0178387 |  2.2430  |  0.7 kB/s  |  0.0 MB/s  |     F:0 P:1      |     F:0 P:1
----------------------------------------------------------------------------------------------------------------------------
[12-08 23:52:54][DEBUG][i-da5b9630]: Done with dd, copied:13 bytes, 0 fullrecords, 1 partrecords - over elapsed:3
[12-08 23:52:54][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1171
[12-08 23:52:54][DEBUG][i-da5b9630]:

[12-08 23:52:54][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:54][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1171.pid
[12-08 23:52:54][DEBUG][i-da5b9630]:

[12-08 23:52:54][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:54][DEBUG][i-da5b9630]: length remaining to write after adding volumeid:19
[12-08 23:52:54][DEBUG][i-da5b9630]:
--->(machine.py:1254)Starting method: dd_monitor(self, ddif=/dev/urandom, ddof=/dev/vdb, ddcount=None, ddbs=19, ddbytes=19, ddcmd=None, ddseek=13, timeout=90, poll_interval=1, tmpfile=None, sync=True)
[12-08 23:52:54][DEBUG][i-da5b9630]: [root@10.111.31.105]# nohup dd if=/dev/urandom of=/dev/vdb seek=13  bs=19  count=1  2> /tmp/eutesterddcmd.10.111.31.105.1174 & echo $! && sleep 2
[12-08 23:52:54][DEBUG][i-da5b9630]:
1115

[12-08 23:52:56][DEBUG][i-da5b9630]: done with exec

----------------------------------------------------------------------------------------------------------------------------
DD DATA INFO                            |     DD TIME INFO    |       DD RATE INFO      |     DD RECORDS FULL/PARTIAL INFO
----------------------------------------------------------------------------------------------------------------------------
BYTES          |      MBs      |  GIGs  | DD TIME  |TEST TIME |  DD RATE   | TEST RATE  |      REC_IN      |     REC_OUT
----------------------------------------------------------------------------------------------------------------------------

19             |      0.0      |  0.0   |0.00924148|  2.2401  |  2.1 kB/s  |  0.0 MB/s  |     F:1 P:0      |     F:1 P:0
----------------------------------------------------------------------------------------------------------------------------
[12-08 23:52:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# sync
[12-08 23:52:57][DEBUG][i-da5b9630]:

[12-08 23:52:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:57][DEBUG][i-da5b9630]: Done with dd, copied:19 bytes, 1 fullrecords, 0 partrecords - over elapsed:3
[12-08 23:52:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1174
[12-08 23:52:57][DEBUG][i-da5b9630]:

[12-08 23:52:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1174.pid
[12-08 23:52:57][DEBUG][i-da5b9630]:

[12-08 23:52:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:52:57][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:52:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:52:57][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:52:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:52:58][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:52:58][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:58][DEBUG][i-da5b9630]: Got MD5 for Volume:vol-0147336f dev:/dev/vdb md5:50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:52:58][DEBUG][i-da5b9630]: Filled Volume:vol-0147336f dev:/dev/vdb md5:50d718e0e7b19f5aa4d67c7af2dc3754
try_to_write_to_disk returned: "True" after 0 minutes 0 seconds.
[12-08 23:52:58][DEBUG][i-da5b9630]: Success attaching volume:vol-0147336f to instance:i-da5b9630, cloud dev:/dev/vde, attached dev:/dev/vdb
[12-08 23:52:58][DEBUG][attach_all_avail_vols_to_instances_in_zones]: Attempting to attach to 0/[Instance:i-da5b9630] instances in zone:one
[12-08 23:52:58][DEBUG][i-da5b9630]: Attempting to attach volume:vol-0b3c3228 to instance:i-da5b9630 to dev:None
[12-08 23:52:58][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:52:58][DEBUG][i-da5b9630]:
vda
vda1
vdb

[12-08 23:52:58][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:58][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:52:58][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:52:58][DEBUG][i-da5b9630]: done with exec
[12-08 23:52:58][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-, status=None, attached_instance=i-da5b9630, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-08 23:52:58][DEBUG][i-da5b9630]: Instance:i-da5b9630 returning available cloud scsi dev:/dev/vdf
[12-08 23:52:58][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2472)Starting method: attach_volume(self, instance=Instance:i-da5b9630, volume=Volume:vol-0b3c3228, device_path=/dev/vdf, pause=10, timeout=480)
[12-08 23:52:58][DEBUG][EC2ops(admin:testrunner)]: Sending attach for Volume:vol-0b3c3228 to be attached to Instance:i-da5b9630 at requested device  /dev/vdf
[12-08 23:52:58][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0b3c3228, state:in-use, attached status:attaching, elapsed:0/480
[12-08 23:53:08][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0b3c3228, Attached: in-use - attached, elapsed:10
[12-08 23:53:08][DEBUG][i-da5b9630]: Checking for volume attachment on guest, elapsed time(0)
[12-08 23:53:08][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:08][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:08][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:08][DEBUG][i-da5b9630]: dev_list_after:vda vda1 vdb vdc
[12-08 23:53:08][DEBUG][i-da5b9630]: Volume:vol-0b3c3228 guest device:/dev/vdc
[12-08 23:53:08][DEBUG][i-da5b9630]: vol-0b3c3228 Requested dev:/dev/vdf, attached to guest device:/dev/vdc
[12-08 23:53:08][DEBUG][i-da5b9630]: vol-0b3c3228Found attached to guest at dev:/dev/vdc, after elapsed:0
Beginning poll loop for result try_to_write_to_disk to go to True
[12-08 23:53:08][DEBUG][i-da5b9630]:
--->(euinstance.py:1445)Starting method: random_fill_volume(self, euvolume=Volume:vol-0b3c3228, srcdev=None, length=32, timepergig=90)
[12-08 23:53:08][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:53:08][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:53:08][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:08][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:08][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:53:08][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/urandom
[12-08 23:53:08][DEBUG][i-da5b9630]:
/dev/urandom

[12-08 23:53:08][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:08][DEBUG][i-da5b9630]:
--->(machine.py:1254)Starting method: dd_monitor(self, ddif=None, ddof=None, ddcount=None, ddbs=1024, ddbytes=None, ddcmd=echo vol-0b3c3228 | dd of=/dev/vdc, ddseek=None, timeout=90, poll_interval=1, tmpfile=None, sync=False)
[12-08 23:53:08][DEBUG][i-da5b9630]: [root@10.111.31.105]# nohup echo vol-0b3c3228 | dd of=/dev/vdc 2> /tmp/eutesterddcmd.10.111.31.105.1188 & echo $! && sleep 2
[12-08 23:53:08][DEBUG][i-da5b9630]:
1142
nohup: ignoring input and redirecting stderr to stdout

[12-08 23:53:10][DEBUG][i-da5b9630]: done with exec

----------------------------------------------------------------------------------------------------------------------------
DD DATA INFO                            |     DD TIME INFO    |       DD RATE INFO      |     DD RECORDS FULL/PARTIAL INFO
----------------------------------------------------------------------------------------------------------------------------
BYTES          |      MBs      |  GIGs  | DD TIME  |TEST TIME |  DD RATE   | TEST RATE  |      REC_IN      |     REC_OUT
----------------------------------------------------------------------------------------------------------------------------

13             |      0.0      |  0.0   |0.0125177 |  2.2374  |  1.0 kB/s  |  0.0 MB/s  |     F:0 P:1      |     F:0 P:1
----------------------------------------------------------------------------------------------------------------------------
[12-08 23:53:12][DEBUG][i-da5b9630]: Done with dd, copied:13 bytes, 0 fullrecords, 1 partrecords - over elapsed:3
[12-08 23:53:12][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1188
[12-08 23:53:12][DEBUG][i-da5b9630]:

[12-08 23:53:12][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:12][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1188.pid
[12-08 23:53:12][DEBUG][i-da5b9630]:

[12-08 23:53:12][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:12][DEBUG][i-da5b9630]: length remaining to write after adding volumeid:19
[12-08 23:53:12][DEBUG][i-da5b9630]:
--->(machine.py:1254)Starting method: dd_monitor(self, ddif=/dev/urandom, ddof=/dev/vdc, ddcount=None, ddbs=19, ddbytes=19, ddcmd=None, ddseek=13, timeout=90, poll_interval=1, tmpfile=None, sync=True)
[12-08 23:53:12][DEBUG][i-da5b9630]: [root@10.111.31.105]# nohup dd if=/dev/urandom of=/dev/vdc seek=13  bs=19  count=1  2> /tmp/eutesterddcmd.10.111.31.105.1192 & echo $! && sleep 2
[12-08 23:53:12][DEBUG][i-da5b9630]:
1153

[12-08 23:53:14][DEBUG][i-da5b9630]: done with exec

----------------------------------------------------------------------------------------------------------------------------
DD DATA INFO                            |     DD TIME INFO    |       DD RATE INFO      |     DD RECORDS FULL/PARTIAL INFO
----------------------------------------------------------------------------------------------------------------------------
BYTES          |      MBs      |  GIGs  | DD TIME  |TEST TIME |  DD RATE   | TEST RATE  |      REC_IN      |     REC_OUT
----------------------------------------------------------------------------------------------------------------------------

19             |      0.0      |  0.0   |0.00921483|  2.2395  |  2.1 kB/s  |  0.0 MB/s  |     F:1 P:0      |     F:1 P:0
----------------------------------------------------------------------------------------------------------------------------
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# sync
[12-08 23:53:15][DEBUG][i-da5b9630]:

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: Done with dd, copied:19 bytes, 1 fullrecords, 0 partrecords - over elapsed:3
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1192
[12-08 23:53:15][DEBUG][i-da5b9630]:

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1192.pid
[12-08 23:53:15][DEBUG][i-da5b9630]:

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:53:15][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:15][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:53:15][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: Got MD5 for Volume:vol-0b3c3228 dev:/dev/vdc md5:3083ca9af96538660455c070def4c410
[12-08 23:53:15][DEBUG][i-da5b9630]: Filled Volume:vol-0b3c3228 dev:/dev/vdc md5:3083ca9af96538660455c070def4c410
try_to_write_to_disk returned: "True" after 0 minutes 0 seconds.
[12-08 23:53:15][DEBUG][i-da5b9630]: Success attaching volume:vol-0b3c3228 to instance:i-da5b9630, cloud dev:/dev/vdf, attached dev:/dev/vdc
[12-08 23:53:15][INFO][attach_all_avail_vols_to_instances_in_zones]:
----------------------------------------------------------------------------------
     - SUCCESS -  TEST:"attach_all_avail_vols_to_instances_in_zones" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:53:15][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        4         11       137
-----------------------------------------------

[12-08 23:53:15][INFO][negative_delete_attached_volumes_in_zones]:
---------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: negative_delete_attached_volumes_in_zones                                  |
 | METHOD:negative_delete_attached_volumes_in_zones, TEST DESCRIPTION:                           |
 |                                                                                               |
 | Description:                                                                                  |
 | Negative test case. Attempts to delete attached volumes for each euinstace                    |
 | in each zone per zone list provided. Confirms that volumes did NOT                            |
 | delete while in use/attached.                                                                 |
 |                                                                                               |
 | End on Failure:False                                                                          |
 | Passing ARGS:""                                                                               |
 | Running test method: "negative_delete_attached_volumes_in_zones(timeout=60, zonelist=None, )" |
 +-----------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------

[12-08 23:53:15][DEBUG][negative_delete_attached_volumes_in_zones]: syncing volumes for instance:i-da5b9630
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:15][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: Checking for volumes whos state is not in sync with our instance's test state...
[12-08 23:53:15][DEBUG][i-da5b9630]: Checking volume:vol-0147336f
[12-08 23:53:15][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0147336f is attached to:i-da5b9630, check for guest dev...
[12-08 23:53:15][DEBUG][i-da5b9630]: Checking any new devs for md5:50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:15][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0147336f
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:15][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:15][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:15][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:15][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:53:15][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0147336f
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:53:16][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:16][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0147336f
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:53:16][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:16][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:16][DEBUG][i-da5b9630]: Found match at dev:/dev/vdb
[12-08 23:53:16][DEBUG][i-da5b9630]: (vol-0147336f)Found dev match. Previous dev:'/dev/vdb', Current dev:'/dev/vdb'
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking volume:vol-0b3c3228
[12-08 23:53:16][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0b3c3228 is attached to:i-da5b9630, check for guest dev...
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking any new devs for md5:3083ca9af96538660455c070def4c410
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:16][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0b3c3228
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:16][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:16][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 3083ca9af96538660455c070def4c410
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0b3c3228
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:53:16][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:16][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 3083ca9af96538660455c070def4c410
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0b3c3228
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:53:16][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:16][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 3083ca9af96538660455c070def4c410
[12-08 23:53:16][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-0b3c3228
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:53:16][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:16][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:53:16][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 3083ca9af96538660455c070def4c410
[12-08 23:53:16][DEBUG][i-da5b9630]: Found match at dev:/dev/vdc
[12-08 23:53:16][DEBUG][i-da5b9630]: (vol-0b3c3228)Found dev match. Previous dev:'/dev/vdc', Current dev:'/dev/vdc'
[12-08 23:53:16][DEBUG][negative_delete_attached_volumes_in_zones]: Success- could not delete attached volume:vol-0147336f
[12-08 23:53:16][DEBUG][negative_delete_attached_volumes_in_zones]: Success- could not delete attached volume:vol-0b3c3228
[12-08 23:53:16][INFO][negative_delete_attached_volumes_in_zones]:
----------------------------------------------------------------------------------
      - SUCCESS -  TEST:"negative_delete_attached_volumes_in_zones" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:53:16][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        5         10       138
-----------------------------------------------

[12-08 23:53:16][INFO][negative_attach_in_use_volume_in_zones]:
-------------------------------------------------------------------------------------------------
 +---------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: negative_attach_in_use_volume_in_zones                                   |
 | METHOD:negative_attach_in_use_volume_in_zones, TEST DESCRIPTION:                            |
 |                                                                                             |
 | Description:                                                                                |
 | Iterates though zones and attempts to attach already attached                               |
 | volumes to instances within each zone.                                                      |
 |                                                                                             |
 | End on Failure:False                                                                        |
 | Passing ARGS:""                                                                             |
 | Running test method: "negative_attach_in_use_volume_in_zones(timeout=480, zonelist=None, )" |
 +---------------------------------------------------------------------------------------------+
-------------------------------------------------------------------------------------------------

[12-08 23:53:16][DEBUG][i-da5b9630]: Attempting to attach volume:vol-0147336f to instance:i-da5b9630 to dev:None
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:16][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:16][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:16][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:17][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:17][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:17][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-, status=None, attached_instance=i-da5b9630, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-08 23:53:17][DEBUG][i-da5b9630]: Instance:i-da5b9630 returning available cloud scsi dev:/dev/vdg
[12-08 23:53:17][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2472)Starting method: attach_volume(self, instance=Instance:i-da5b9630, volume=Volume:vol-0147336f, device_path=/dev/vdg, pause=10, timeout=480)
[12-08 23:53:17][DEBUG][EC2ops(admin:testrunner)]: Sending attach for Volume:vol-0147336f to be attached to Instance:i-da5b9630 at requested device  /dev/vdg
[12-08 23:53:17][DEBUG][negative_attach_in_use_volume_in_zones]: negative_attach_in_use_volume_in_zones Passed. Could not attach in-use volume
[12-08 23:53:17][DEBUG][i-da5b9630]: Attempting to attach volume:vol-0b3c3228 to instance:i-da5b9630 to dev:None
[12-08 23:53:17][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:17][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:17][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:17][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:17][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:17][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:17][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-, status=None, attached_instance=i-da5b9630, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-08 23:53:17][DEBUG][i-da5b9630]: Instance:i-da5b9630 returning available cloud scsi dev:/dev/vdg
[12-08 23:53:17][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2472)Starting method: attach_volume(self, instance=Instance:i-da5b9630, volume=Volume:vol-0b3c3228, device_path=/dev/vdg, pause=10, timeout=480)
[12-08 23:53:17][DEBUG][EC2ops(admin:testrunner)]: Sending attach for Volume:vol-0b3c3228 to be attached to Instance:i-da5b9630 at requested device  /dev/vdg
[12-08 23:53:18][DEBUG][negative_attach_in_use_volume_in_zones]: negative_attach_in_use_volume_in_zones Passed. Could not attach in-use volume
[12-08 23:53:18][INFO][negative_attach_in_use_volume_in_zones]:
----------------------------------------------------------------------------------
       - SUCCESS -  TEST:"negative_attach_in_use_volume_in_zones" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:53:18][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        6         9        139
-----------------------------------------------

[12-08 23:53:18][INFO][create_vols_per_zone]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: create_vols_per_zone                                                                   |
 | METHOD:create_vols_per_zone, TEST DESCRIPTION:                                                            |
 |                                                                                                           |
 | Description:                                                                                              |
 | Intention of this test is to verify creation of volume(s) per zone given.                                 |
 | Upon successful creation the volumes will be appended to a volumes list                                   |
 | for the zone it was created in.                                                                           |
 | These volumes may be later used if in later ebstests suite tests.                                         |
 |                                                                                                           |
 | End on Failure:False                                                                                      |
 | Passing ARGS:""                                                                                           |
 | Running test method: "create_vols_per_zone(timepergig=300, snapshot=None, volsperzone=1, zonelist=None,   |
 | size=1, )"                                                                                                |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1916)Starting method: create_volumes(self, zone=one, size=1, count=1, mincount=None, eof=True, monitor_to_state=available, delay=0, snapshot=None, timeout=0, poll_interval=10, timepergig=300)
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: Sending create volume request, count:1
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: 1/1 requests for volume creation succeeded.
[12-08 23:53:18][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+----+----+--------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS|AGE |SIZE|SRC_SNAP| MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+----+----+--------+-----------+----+--------+
|vol-abf37b3d|  0  | creating |0.08| 1  |  None  |None/(1024)|one |  None  |
+------------+-----+----------+----+----+--------+-----------+----+--------+

[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2015)Starting method: monitor_created_euvolumes_to_state(self, volumes=[Volume:vol-abf37b3d], eof=True, mincount=1, state=available, poll_interval=10, deletefailed=True, size=1, timepergig=300)
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: Polling 1 volumes for status:"available"...
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-abf37b3d) State(creating), seconds elapsed: 0/300
[12-08 23:53:18][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:0, Waiting on 1 volumes to enter state:available-----
[12-08 23:53:28][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-abf37b3d) State(available), seconds elapsed: 10/300
[12-08 23:53:28][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:10, Waiting on 0 volumes to enter state:available-----
[12-08 23:53:28][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+-----+----+--------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS| AGE |SIZE|SRC_SNAP| MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+-----+----+--------+-----------+----+--------+
|vol-abf37b3d|  0  |available |10.17| 1  |  None  |None/(1024)|one |  None  |
+------------+-----+----------+-----+----+--------+-----------+----+--------+

[12-08 23:53:28][DEBUG][create_vols_per_zone]: create_vols_per_zone created vols(1) zone:one
[12-08 23:53:28][INFO][create_vols_per_zone]:
----------------------------------------------------------------------------------
                - SUCCESS -  TEST:"create_vols_per_zone" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:53:28][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        7         8        149
-----------------------------------------------

[12-08 23:53:28][INFO][attach_all_avail_vols_to_instances_in_zones]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: attach_all_avail_vols_to_instances_in_zones                                            |
 | METHOD:attach_all_avail_vols_to_instances_in_zones, TEST DESCRIPTION:                                     |
 |                                                                                                           |
 | Description:                                                                                              |
 | Iterates though zones and attempts to attach volumes to an instance                                       |
 | within each zone.                                                                                         |
 |                                                                                                           |
 | :parram zonelist: list of zones to include in test                                                        |
 | :param timeout: timeout used for attach volume method                                                     |
 | :param overwrite: boolean to indicate whether a non-zero filled volume should have                        |
 | new unique data prepended for md5sum.                                                                     |
 | This should be used when zero fill volume property is not in use                                          |
 | upon volume first attach. It should not be used after the 1st attach                                      |
 | and volume has been converted to a euvolume within this test.                                             |
 |                                                                                                           |
 | End on Failure:False                                                                                      |
 | Passing ARGS:                                                                                             |
 | ---------------------                                                                                     |
 | overwrite : True                                                                                          |
 | ---------------------                                                                                     |
 | Running test method: "attach_all_avail_vols_to_instances_in_zones(timeout=480, zonelist=None,             |
 | overwrite=True, )"                                                                                        |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

KWARG:overwrite = True
[12-08 23:53:28][DEBUG][attach_all_avail_vols_to_instances_in_zones]: Attempting to attach to 0/[Instance:i-da5b9630] instances in zone:one
[12-08 23:53:28][DEBUG][i-da5b9630]: Attempting to attach volume:vol-abf37b3d to instance:i-da5b9630 to dev:None
[12-08 23:53:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:28][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc

[12-08 23:53:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:28][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:28][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-, status=None, attached_instance=i-da5b9630, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-08 23:53:28][DEBUG][i-da5b9630]: Instance:i-da5b9630 returning available cloud scsi dev:/dev/vdg
[12-08 23:53:28][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2472)Starting method: attach_volume(self, instance=Instance:i-da5b9630, volume=Volume:vol-abf37b3d, device_path=/dev/vdg, pause=10, timeout=480)
[12-08 23:53:28][DEBUG][EC2ops(admin:testrunner)]: Sending attach for Volume:vol-abf37b3d to be attached to Instance:i-da5b9630 at requested device  /dev/vdg
[12-08 23:53:29][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-abf37b3d, state:in-use, attached status:attaching, elapsed:0/480
[12-08 23:53:39][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-abf37b3d, state:in-use, attached status:attaching, elapsed:10/480
[12-08 23:53:49][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-abf37b3d, Attached: in-use - attached, elapsed:20
[12-08 23:53:49][DEBUG][i-da5b9630]: Checking for volume attachment on guest, elapsed time(0)
[12-08 23:53:49][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:49][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:53:49][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:49][DEBUG][i-da5b9630]: dev_list_after:vda vda1 vdb vdc vdd
[12-08 23:53:49][DEBUG][i-da5b9630]: Volume:vol-abf37b3d guest device:/dev/vdd
[12-08 23:53:49][DEBUG][i-da5b9630]: vol-abf37b3d Requested dev:/dev/vdg, attached to guest device:/dev/vdd
[12-08 23:53:49][DEBUG][i-da5b9630]: vol-abf37b3dFound attached to guest at dev:/dev/vdd, after elapsed:0
Beginning poll loop for result try_to_write_to_disk to go to True
[12-08 23:53:49][DEBUG][i-da5b9630]:
--->(euinstance.py:1445)Starting method: random_fill_volume(self, euvolume=Volume:vol-abf37b3d, srcdev=None, length=32, timepergig=90)
[12-08 23:53:49][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdd
[12-08 23:53:49][DEBUG][i-da5b9630]:
/dev/vdd

[12-08 23:53:49][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:49][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:49][DEBUG][i-da5b9630]: File /dev/vdd is present on i-da5b9630
[12-08 23:53:49][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/urandom
[12-08 23:53:49][DEBUG][i-da5b9630]:
/dev/urandom

[12-08 23:53:49][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:49][DEBUG][i-da5b9630]:
--->(machine.py:1254)Starting method: dd_monitor(self, ddif=None, ddof=None, ddcount=None, ddbs=1024, ddbytes=None, ddcmd=echo vol-abf37b3d | dd of=/dev/vdd, ddseek=None, timeout=90, poll_interval=1, tmpfile=None, sync=False)
[12-08 23:53:49][DEBUG][i-da5b9630]: [root@10.111.31.105]# nohup echo vol-abf37b3d | dd of=/dev/vdd 2> /tmp/eutesterddcmd.10.111.31.105.1229 & echo $! && sleep 2
[12-08 23:53:49][DEBUG][i-da5b9630]:
1225
nohup: ignoring input and redirecting stderr to stdout

[12-08 23:53:51][DEBUG][i-da5b9630]: done with exec

----------------------------------------------------------------------------------------------------------------------------
DD DATA INFO                            |     DD TIME INFO    |       DD RATE INFO      |     DD RECORDS FULL/PARTIAL INFO
----------------------------------------------------------------------------------------------------------------------------
BYTES          |      MBs      |  GIGs  | DD TIME  |TEST TIME |  DD RATE   | TEST RATE  |      REC_IN      |     REC_OUT
----------------------------------------------------------------------------------------------------------------------------

13             |      0.0      |  0.0   |0.0124508 |  2.2392  |  1.0 kB/s  |  0.0 MB/s  |     F:0 P:1      |     F:0 P:1
----------------------------------------------------------------------------------------------------------------------------
[12-08 23:53:52][DEBUG][i-da5b9630]: Done with dd, copied:13 bytes, 0 fullrecords, 1 partrecords - over elapsed:3
[12-08 23:53:52][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1229
[12-08 23:53:52][DEBUG][i-da5b9630]:

[12-08 23:53:52][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:52][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1229.pid
[12-08 23:53:52][DEBUG][i-da5b9630]:

[12-08 23:53:52][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:52][DEBUG][i-da5b9630]: length remaining to write after adding volumeid:19
[12-08 23:53:52][DEBUG][i-da5b9630]:
--->(machine.py:1254)Starting method: dd_monitor(self, ddif=/dev/urandom, ddof=/dev/vdd, ddcount=None, ddbs=19, ddbytes=19, ddcmd=None, ddseek=13, timeout=90, poll_interval=1, tmpfile=None, sync=True)
[12-08 23:53:52][DEBUG][i-da5b9630]: [root@10.111.31.105]# nohup dd if=/dev/urandom of=/dev/vdd seek=13  bs=19  count=1  2> /tmp/eutesterddcmd.10.111.31.105.1232 & echo $! && sleep 2
[12-08 23:53:52][DEBUG][i-da5b9630]:
1236

[12-08 23:53:54][DEBUG][i-da5b9630]: done with exec

----------------------------------------------------------------------------------------------------------------------------
DD DATA INFO                            |     DD TIME INFO    |       DD RATE INFO      |     DD RECORDS FULL/PARTIAL INFO
----------------------------------------------------------------------------------------------------------------------------
BYTES          |      MBs      |  GIGs  | DD TIME  |TEST TIME |  DD RATE   | TEST RATE  |      REC_IN      |     REC_OUT
----------------------------------------------------------------------------------------------------------------------------

19             |      0.0      |  0.0   |0.00907998|  2.2409  |  2.1 kB/s  |  0.0 MB/s  |     F:1 P:0      |     F:1 P:0
----------------------------------------------------------------------------------------------------------------------------
[12-08 23:53:55][DEBUG][i-da5b9630]: [root@10.111.31.105]# sync
[12-08 23:53:55][DEBUG][i-da5b9630]:

[12-08 23:53:55][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:55][DEBUG][i-da5b9630]: Done with dd, copied:19 bytes, 1 fullrecords, 0 partrecords - over elapsed:3
[12-08 23:53:55][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1232
[12-08 23:53:56][DEBUG][i-da5b9630]:

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# rm -f /tmp/eutesterddcmd.10.111.31.105.1232.pid
[12-08 23:53:56][DEBUG][i-da5b9630]:

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdd
[12-08 23:53:56][DEBUG][i-da5b9630]:
/dev/vdd

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:56][DEBUG][i-da5b9630]: File /dev/vdd is present on i-da5b9630
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdd | md5sum
[12-08 23:53:56][DEBUG][i-da5b9630]:
60ee9fafc4174de3853b47d199750c3e  -

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: Got MD5 for Volume:vol-abf37b3d dev:/dev/vdd md5:60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:56][DEBUG][i-da5b9630]: Filled Volume:vol-abf37b3d dev:/dev/vdd md5:60ee9fafc4174de3853b47d199750c3e
try_to_write_to_disk returned: "True" after 0 minutes 0 seconds.
[12-08 23:53:56][DEBUG][i-da5b9630]: Success attaching volume:vol-abf37b3d to instance:i-da5b9630, cloud dev:/dev/vdg, attached dev:/dev/vdd
[12-08 23:53:56][INFO][attach_all_avail_vols_to_instances_in_zones]:
----------------------------------------------------------------------------------
     - SUCCESS -  TEST:"attach_all_avail_vols_to_instances_in_zones" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:53:56][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        8         7        176
-----------------------------------------------

[12-08 23:53:56][INFO][reboot_instances_in_zone_verify_volumes]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: reboot_instances_in_zone_verify_volumes                                                |
 | METHOD:reboot_instances_in_zone_verify_volumes, TEST DESCRIPTION:                                         |
 |                                                                                                           |
 | Description:                                                                                              |
 | Attempts to iterate through each instance in each zone and reboot the                                     |
 | instance(s).                                                                                              |
 | Attempts to verify the attached volume state post reboot.                                                 |
 |                                                                                                           |
 | End on Failure:False                                                                                      |
 | Passing ARGS:                                                                                             |
 | ---------------------                                                                                     |
 | waitconnect : 30                                                                                          |
 | ---------------------                                                                                     |
 | Running test method: "reboot_instances_in_zone_verify_volumes(waitconnect=30, timeout=480, zonelist=None, |
 | )"                                                                                                        |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

KWARG:waitconnect = 30
[12-08 23:53:56][DEBUG][i-da5b9630]: Attempting to reboot instance:i-da5b9630, check attached volume state first
Beginning poll loop for result get_safe_uptime to go to None
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# cat /proc/uptime
[12-08 23:53:56][DEBUG][i-da5b9630]:
94.97 170.17

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
get_safe_uptime returned: "94" after 0 minutes 0 seconds.
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:56][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking for volumes whos state is not in sync with our instance's test state...
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking volume:vol-0147336f
[12-08 23:53:56][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0147336f is attached to:i-da5b9630, check for guest dev...
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking any new devs for md5:50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:56][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0147336f
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:56][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:56][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:53:56][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0147336f
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:53:56][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:56][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:53:56][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0147336f
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:53:56][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:56][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:53:56][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:53:56][DEBUG][i-da5b9630]: Found match at dev:/dev/vdb
[12-08 23:53:56][DEBUG][i-da5b9630]: (vol-0147336f)Found dev match. Previous dev:'/dev/vdb', Current dev:'/dev/vdb'
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking volume:vol-0b3c3228
[12-08 23:53:56][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0b3c3228 is attached to:i-da5b9630, check for guest dev...
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking any new devs for md5:3083ca9af96538660455c070def4c410
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:56][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0b3c3228
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:56][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:56][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:53:56][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 3083ca9af96538660455c070def4c410
[12-08 23:53:56][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0b3c3228
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:53:56][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:53:56][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:56][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:56][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:53:56][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 3083ca9af96538660455c070def4c410
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0b3c3228
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 3083ca9af96538660455c070def4c410
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-0b3c3228
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 3083ca9af96538660455c070def4c410
[12-08 23:53:57][DEBUG][i-da5b9630]: Found match at dev:/dev/vdc
[12-08 23:53:57][DEBUG][i-da5b9630]: (vol-0b3c3228)Found dev match. Previous dev:'/dev/vdc', Current dev:'/dev/vdc'
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking volume:vol-abf37b3d
[12-08 23:53:57][DEBUG][i-da5b9630]: Cloud beleives volume:vol-abf37b3d is attached to:i-da5b9630, check for guest dev...
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking any new devs for md5:60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:53:57][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-abf37b3d
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-abf37b3d
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-abf37b3d
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-abf37b3d
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:57][DEBUG][i-da5b9630]: Checking /dev/vdd for match against euvolume:vol-abf37b3d
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdd
[12-08 23:53:57][DEBUG][i-da5b9630]:
/dev/vdd

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: exit code:0
[12-08 23:53:57][DEBUG][i-da5b9630]: File /dev/vdd is present on i-da5b9630
[12-08 23:53:57][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdd | md5sum
[12-08 23:53:57][DEBUG][i-da5b9630]:
60ee9fafc4174de3853b47d199750c3e  -

[12-08 23:53:57][DEBUG][i-da5b9630]: done with exec
[12-08 23:53:57][DEBUG][i-da5b9630]: comparing 60ee9fafc4174de3853b47d199750c3e vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:53:57][DEBUG][i-da5b9630]: Found match at dev:/dev/vdd
[12-08 23:53:57][DEBUG][i-da5b9630]: (vol-abf37b3d)Found dev match. Previous dev:'/dev/vdd', Current dev:'/dev/vdd'
[12-08 23:53:57][DEBUG][i-da5b9630]: Rebooting now...
[12-08 23:54:27][INFO][i-da5b9630]: Attempting to reconnect_to_instance:i-da5b9630
[12-08 23:54:27][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:54:28][DEBUG][i-da5b9630]: Found local file for ssh keypair, setting keypath to:/mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][i-da5b9630]: reset_ssh_connection for:i-da5b9630
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:54:28][DEBUG][i-da5b9630]: Found local file for ssh keypair, setting keypath to:/mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][i-da5b9630]: Connecting ssh i-da5b9630
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:484)Starting method: get_all_current_local_keys(self, key_name=LegacyEbsTestSuite_1481241111, path=./, extension=.pem)
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Checking local path: ./ for keyfile: LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Found key at path:./LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][EC2ops(admin:testrunner)]: Found file with matching finger print for key:LegacyEbsTestSuite_1481241111
[12-08 23:54:28][DEBUG][i-da5b9630]: Found local file for ssh keypair, setting keypath to:/mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][i-da5b9630]: SSH connection has hostname:10.111.31.105 user:root and keypath: /mnt/swathi/dev-swathi-EUCA-12910/97/nephoria/LegacyEbsTestSuite_1481241111.pem
[12-08 23:54:28][DEBUG][i-da5b9630]: SSH connection attempt(1 of 3), host:'root@10.111.31.105', using ipv4:10.111.31.105, thru proxy:'None'
[12-08 23:54:28][DEBUG][i-da5b9630]: SSH - Connected to 10.111.31.105
[12-08 23:54:28][DEBUG][i-da5b9630]: Try some sys...
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]#
[12-08 23:54:28][DEBUG][i-da5b9630]:

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:28][INFO][i-da5b9630]: SSH Connection Succeeded
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:28][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
Beginning poll loop for result get_safe_uptime to go to None
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# cat /proc/uptime
[12-08 23:54:28][DEBUG][i-da5b9630]:
21.05 35.80

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
get_safe_uptime returned: "21" after 0 minutes 0 seconds.
[12-08 23:54:28][DEBUG][i-da5b9630]: Instance uptime indicates a reboot. Orig:94, New:21, elapsed:32
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:28][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:28][DEBUG][i-da5b9630]: Checking for volumes whos state is not in sync with our instance's test state...
[12-08 23:54:28][DEBUG][i-da5b9630]: Checking volume:vol-0147336f
[12-08 23:54:28][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0147336f is attached to:i-da5b9630, check for guest dev...
[12-08 23:54:28][DEBUG][i-da5b9630]: Checking any new devs for md5:50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:28][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:28][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0147336f
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:28][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:28][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:28][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:54:28][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:54:28][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:28][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:28][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0147336f
[12-08 23:54:28][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0147336f
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:29][DEBUG][i-da5b9630]: Found match at dev:/dev/vdb
[12-08 23:54:29][DEBUG][i-da5b9630]: (vol-0147336f)Found dev match. Previous dev:'/dev/vdb', Current dev:'/dev/vdb'
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking volume:vol-0b3c3228
[12-08 23:54:29][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0b3c3228 is attached to:i-da5b9630, check for guest dev...
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking any new devs for md5:3083ca9af96538660455c070def4c410
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:29][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0b3c3228
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 3083ca9af96538660455c070def4c410
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0b3c3228
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 3083ca9af96538660455c070def4c410
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0b3c3228
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 3083ca9af96538660455c070def4c410
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-0b3c3228
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 3083ca9af96538660455c070def4c410
[12-08 23:54:29][DEBUG][i-da5b9630]: Found match at dev:/dev/vdc
[12-08 23:54:29][DEBUG][i-da5b9630]: (vol-0b3c3228)Found dev match. Previous dev:'/dev/vdc', Current dev:'/dev/vdc'
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking volume:vol-abf37b3d
[12-08 23:54:29][DEBUG][i-da5b9630]: Cloud beleives volume:vol-abf37b3d is attached to:i-da5b9630, check for guest dev...
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking any new devs for md5:60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:29][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-abf37b3d
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-abf37b3d
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:54:29][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:29][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:54:29][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:54:29][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:29][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:29][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-abf37b3d
[12-08 23:54:29][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-abf37b3d
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vdd for match against euvolume:vol-abf37b3d
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdd
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vdd

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vdd is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdd | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
60ee9fafc4174de3853b47d199750c3e  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 60ee9fafc4174de3853b47d199750c3e vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:30][DEBUG][i-da5b9630]: Found match at dev:/dev/vdd
[12-08 23:54:30][DEBUG][i-da5b9630]: (vol-abf37b3d)Found dev match. Previous dev:'/dev/vdd', Current dev:'/dev/vdd'
[12-08 23:54:30][DEBUG][i-da5b9630]: i-da5b9630 reboot_instance_and_verify Success
[12-08 23:54:30][INFO][reboot_instances_in_zone_verify_volumes]:
----------------------------------------------------------------------------------
       - SUCCESS -  TEST:"reboot_instances_in_zone_verify_volumes" COMPLETE
----------------------------------------------------------------------------------

[12-08 23:54:30][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      0        9         6        210
-----------------------------------------------

[12-08 23:54:30][INFO][detach_volumes_in_zones]:
---------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: detach_volumes_in_zones                                                          |
 | METHOD:detach_volumes_in_zones, TEST DESCRIPTION:                                                   |
 |                                                                                                     |
 | Description:                                                                                        |
 | Attempts to detach volcount volumes from each instance in the provided                              |
 | zonelist.                                                                                           |
 | If volcount is None or 0, will attempt to detach all volumes from all                               |
 | instances.                                                                                          |
 | Attempts to verify detached volume state on both the cloud and the guest                            |
 | by default will attempt to detach a single volume from each instance                                |
 |                                                                                                     |
 | End on Failure:False                                                                                |
 | Passing ARGS:""                                                                                     |
 | Running test method: "detach_volumes_in_zones(volcount=1, timeout=480, zonelist=None, eof=False, )" |
 +-----------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------

[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:30][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking for volumes whos state is not in sync with our instance's test state...
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking volume:vol-0147336f
[12-08 23:54:30][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0147336f is attached to:i-da5b9630, check for guest dev...
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking any new devs for md5:50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:30][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0147336f
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0147336f
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0147336f
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 50d718e0e7b19f5aa4d67c7af2dc3754
[12-08 23:54:30][DEBUG][i-da5b9630]: Found match at dev:/dev/vdb
[12-08 23:54:30][DEBUG][i-da5b9630]: (vol-0147336f)Found dev match. Previous dev:'/dev/vdb', Current dev:'/dev/vdb'
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking volume:vol-0b3c3228
[12-08 23:54:30][DEBUG][i-da5b9630]: Cloud beleives volume:vol-0b3c3228 is attached to:i-da5b9630, check for guest dev...
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking any new devs for md5:3083ca9af96538660455c070def4c410
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:30][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-0b3c3228
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:30][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:30][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:54:30][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:54:30][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:30][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 3083ca9af96538660455c070def4c410
[12-08 23:54:30][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-0b3c3228
[12-08 23:54:30][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 3083ca9af96538660455c070def4c410
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-0b3c3228
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 3083ca9af96538660455c070def4c410
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-0b3c3228
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 3083ca9af96538660455c070def4c410
[12-08 23:54:31][DEBUG][i-da5b9630]: Found match at dev:/dev/vdc
[12-08 23:54:31][DEBUG][i-da5b9630]: (vol-0b3c3228)Found dev match. Previous dev:'/dev/vdc', Current dev:'/dev/vdc'
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking volume:vol-abf37b3d
[12-08 23:54:31][DEBUG][i-da5b9630]: Cloud beleives volume:vol-abf37b3d is attached to:i-da5b9630, check for guest dev...
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking any new devs for md5:60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-08 23:54:31][DEBUG][i-da5b9630]:
vda
vda1
vdb
vdc
vdd

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vda for match against euvolume:vol-abf37b3d
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vda

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vda is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
137d243478c2c39e53156ed5f21d5f0f  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 137d243478c2c39e53156ed5f21d5f0f vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vda1 for match against euvolume:vol-abf37b3d
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vda1
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vda1

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vda1 is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vda1 | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
70bc8f4b72a86921468bf8e8441dce51  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 70bc8f4b72a86921468bf8e8441dce51 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vdb for match against euvolume:vol-abf37b3d
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdb
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vdb

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vdb is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdb | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
50d718e0e7b19f5aa4d67c7af2dc3754  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 50d718e0e7b19f5aa4d67c7af2dc3754 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vdc for match against euvolume:vol-abf37b3d
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdc
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vdc

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vdc is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdc | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
3083ca9af96538660455c070def4c410  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 3083ca9af96538660455c070def4c410 vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:31][DEBUG][i-da5b9630]: Checking /dev/vdd for match against euvolume:vol-abf37b3d
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls /dev/vdd
[12-08 23:54:31][DEBUG][i-da5b9630]:
/dev/vdd

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: exit code:0
[12-08 23:54:31][DEBUG][i-da5b9630]: File /dev/vdd is present on i-da5b9630
[12-08 23:54:31][DEBUG][i-da5b9630]: [root@10.111.31.105]# head -c 32 /dev/vdd | md5sum
[12-08 23:54:31][DEBUG][i-da5b9630]:
60ee9fafc4174de3853b47d199750c3e  -

[12-08 23:54:31][DEBUG][i-da5b9630]: done with exec
[12-08 23:54:31][DEBUG][i-da5b9630]: comparing 60ee9fafc4174de3853b47d199750c3e vs 60ee9fafc4174de3853b47d199750c3e
[12-08 23:54:31][DEBUG][i-da5b9630]: Found match at dev:/dev/vdd
[12-08 23:54:31][DEBUG][i-da5b9630]: (vol-abf37b3d)Found dev match. Previous dev:'/dev/vdd', Current dev:'/dev/vdd'
[12-08 23:54:32][DEBUG][EC2ops(admin:testrunner)]: Sent detach for volume: vol-0147336f which is currently in state: in-use
[12-08 23:54:32][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:0
[12-08 23:54:42][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:10
[12-08 23:54:52][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:20
[12-08 23:55:02][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:30
[12-08 23:55:12][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:40
[12-08 23:55:22][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:50
[12-08 23:55:32][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:60
[12-08 23:55:42][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:70
[12-08 23:55:52][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:80
[12-08 23:56:02][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:90
[12-08 23:56:12][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:100
[12-08 23:56:22][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:110
[12-08 23:56:32][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:120
[12-08 23:56:42][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:130
[12-08 23:56:52][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:140
[12-08 23:57:02][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:150
[12-08 23:57:12][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:160
[12-08 23:57:22][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:170
[12-08 23:57:32][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:180
[12-08 23:57:42][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:190
[12-08 23:57:52][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:200
[12-08 23:58:03][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:210
[12-08 23:58:13][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:220
[12-08 23:58:23][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:231
[12-08 23:58:33][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:241
[12-08 23:58:43][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:251
[12-08 23:58:53][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:261
[12-08 23:59:03][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:271
[12-08 23:59:13][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:281
[12-08 23:59:23][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:291
[12-08 23:59:33][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:301
[12-08 23:59:43][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:311
[12-08 23:59:53][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:321
[12-09 00:00:03][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:331
[12-09 00:00:13][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:341
[12-09 00:00:23][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:351
[12-09 00:00:33][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:361
[12-09 00:00:43][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:371
[12-09 00:00:53][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:381
[12-09 00:01:03][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:391
[12-09 00:01:13][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:401
[12-09 00:01:23][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:411
[12-09 00:01:33][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:421
[12-09 00:01:43][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:431
[12-09 00:01:54][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:441
[12-09 00:02:04][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:451
[12-09 00:02:14][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:462
[12-09 00:02:24][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f state:in-use, attached_data:detaching, pause:10, instance:i-da5b9630, elapsed:472
[12-09 00:02:34][DEBUG][detach_volumes_in_zones]: fail. Could not detach Volume:vol-0147336ffrom instance:i-da5b9630
Traceback (most recent call last):
  File "build/bdist.linux-x86_64/egg/nephoria/testcase_utils/cli_test_runner.py", line 263, in run
    ret = self.method(*args, **kwargs)
  File "nephoria/testcases/ec2/ebs/legacy_ebs_test_suite.py", line 545, in detach_volumes_in_zones
    raise Exception(errmsg)
Exception:
Could not detach Volume:vol-0147336ffrom instance:i-da5b9630,err:vol-0147336f:DETACH FAILED - Volume status remained at:in-use, attach_data_status:detaching, instance: i-da5b9630


[12-09 00:02:34][INFO][detach_volumes_in_zones]:
----------------------------------------------------------------------------------
               - FAILURE -  TEST:"detach_volumes_in_zones" COMPLETE
----------------------------------------------------------------------------------

[12-09 00:02:34][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      1        9         5        693
-----------------------------------------------

[12-09 00:02:34][INFO][create_snapshots_all_vols_in_zone]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: create_snapshots_all_vols_in_zone                                                      |
 | METHOD:create_snapshots_all_vols_in_zone, TEST DESCRIPTION:                                               |
 |                                                                                                           |
 | Description:                                                                                              |
 | Attempts to iterate through each zone in zonelist, and create a snapshot                                  |
 | from each volume in the zone's volume list who's state matches volstate                                   |
 |                                                                                                           |
 |                                                                                                           |
 | End on Failure:False                                                                                      |
 | Passing ARGS:                                                                                             |
 | ---------------------                                                                                     |
 | wait_on_progress : 20                                                                                     |
 | ---------------------                                                                                     |
 | Running test method: "create_snapshots_all_vols_in_zone(volstate=all, zonelist=None, wait_on_progress=20, |
 | )"                                                                                                        |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

KWARG:wait_on_progress = 20
[12-09 00:02:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2646)Starting method: create_snapshot_from_volume(self, volume=Volume:vol-0147336f, wait_on_progress=20, poll_interval=10, timeout=0, description=ebstest)
[12-09 00:02:34][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2730)Starting method: create_snapshots(self, volume=Volume:vol-0147336f, count=1, mincount=1, eof=True, delay=0, wait_on_progress=20, poll_count=48, poll_interval=10, timeout=0, monitor_to_completed=True, delete_failed=True, description=ebstest)
[12-09 00:02:34][DEBUG][EC2ops(admin:testrunner)]: Create_snapshots count:1, mincount:1, wait_on_progress:20,eof:True
[12-09 00:02:35][DEBUG][EC2ops(admin:testrunner)]: Attempting to create snapshot #0, id:snap-2c78fc2e
[12-09 00:02:35][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2854)Starting method: monitor_eusnaps_to_completed(self, snaps=[Snapshot:snap-2c78fc2e], mincount=1, eof=True, wait_on_progress=20, poll_count=48, poll_interval=10, timeout=0, monitor_to_progress=None, delete_failed=True)
[12-09 00:02:35][DEBUG][EC2ops(admin:testrunner)]: Monitor_snapshot_to_completed starting...
[12-09 00:02:35][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to go to completed state...
[12-09 00:02:35][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-09 00:02:35][DEBUG][EC2ops(admin:testrunner)]: snap-2c78fc2e, Status:pending, Progress:0%, Polls w/o progress:1/20, Time Elapsed:0/0
[12-09 00:02:45][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-09 00:02:45][DEBUG][EC2ops(admin:testrunner)]: snap-2c78fc2e, Status:pending, Progress:25%, Polls w/o progress:0/20, Time Elapsed:10/0
[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]: snap-2c78fc2e, Status:completed, Progress:100%, Polls w/o progress:0/20, Time Elapsed:20/0
[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]: snap-2c78fc2e created after 20 seconds. Status:completed, Progress:100%
[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3044)Starting method: delete_snapshots(self, snapshots=[], valid_states=completed,failed, base_timeout=60, add_time_per_snap=10, wait_for_valid_state=120, poll_interval=10, eof=False)
[12-09 00:02:55][INFO][EC2ops(admin:testrunner)]:
+---------------+-------+---------+---------+-----+-----------+--------------------+---------------------------------------+----------+
|    SNAP_ID    | ORDER | CMDTIME | ELAPSED |  %  |   STATUS  |   SRC_VOL:(ZONE)   |             SRC_MD5:(LEN)             | INFO_MSG |
+---------------+-------+---------+---------+-----+-----------+--------------------+---------------------------------------+----------+
| snap-2c78fc2e |   0   |   0.79  |    20   | 100 | completed | vol-0147336f:(one) | 50d718e0e7b19f5aa4d67c7af2dc3754:(32) | SUCCESS  |
+---------------+-------+---------+---------+-----+-----------+--------------------+---------------------------------------+----------+

[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2646)Starting method: create_snapshot_from_volume(self, volume=Volume:vol-0b3c3228, wait_on_progress=20, poll_interval=10, timeout=0, description=ebstest)
[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2730)Starting method: create_snapshots(self, volume=Volume:vol-0b3c3228, count=1, mincount=1, eof=True, delay=0, wait_on_progress=20, poll_count=48, poll_interval=10, timeout=0, monitor_to_completed=True, delete_failed=True, description=ebstest)
[12-09 00:02:55][DEBUG][EC2ops(admin:testrunner)]: Create_snapshots count:1, mincount:1, wait_on_progress:20,eof:True
[12-09 00:02:56][DEBUG][EC2ops(admin:testrunner)]: Attempting to create snapshot #0, id:snap-a0e3a15a
[12-09 00:02:56][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2854)Starting method: monitor_eusnaps_to_completed(self, snaps=[Snapshot:snap-a0e3a15a], mincount=1, eof=True, wait_on_progress=20, poll_count=48, poll_interval=10, timeout=0, monitor_to_progress=None, delete_failed=True)
[12-09 00:02:56][DEBUG][EC2ops(admin:testrunner)]: Monitor_snapshot_to_completed starting...
[12-09 00:02:56][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to go to completed state...
[12-09 00:02:56][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-09 00:02:56][DEBUG][EC2ops(admin:testrunner)]: snap-a0e3a15a, Status:pending, Progress:0%, Polls w/o progress:1/20, Time Elapsed:0/0
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Waiting for 1 snapshots to complete creation
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Exception caught in snapshot creation, snapshot:snap-a0e3a15a.Err:
Traceback (most recent call last):
  File "build/bdist.linux-x86_64/egg/nephoria/aws/ec2/ec2ops.py", line 2916, in monitor_eusnaps_to_completed
    " progress:"+snapshot.progress+")")
Exception: Snapshot:snap-a0e3a15a failed after Polling(2) ,Waited(0 sec), last reported (status:failed progress:0%)

Snapshot:snap-a0e3a15a failed after Polling(2) ,Waited(0 sec), last reported (status:failed progress:0%)
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3044)Starting method: delete_snapshots(self, snapshots=[Snapshot:snap-a0e3a15a], valid_states=completed,failed, base_timeout=60, add_time_per_snap=10, wait_for_valid_state=120, poll_interval=10, eof=False)
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2995)Starting method: get_snapshots(self, snapid=snap-a0e3a15a, volume_id=None, volume_size=None, volume_md5=None, filters=None, maxcount=1, owner_id=None)
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Snapshot:snap-92bbc7b1 no longer found on system
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Checking snap:snap-92bbc7b1 for match...
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Checking snapshot:snap-a0e3a15a status:failed
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Waiting for remaining 1 snaps to delete...
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Snapshot not found, assuming it's already deleted:snap-a0e3a15a
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Snapshot:snap-a0e3a15a is deleted
Traceback (most recent call last):
  File "build/bdist.linux-x86_64/egg/nephoria/testcase_utils/cli_test_runner.py", line 263, in run
    ret = self.method(*args, **kwargs)
  File "nephoria/testcases/ec2/ebs/legacy_ebs_test_suite.py", line 633, in create_snapshots_all_vols_in_zone
    volume, description="ebstest", wait_on_progress=wait_on_progress)
  File "/mnt/swathi/dev-swathi-EUCA-12910/97/envnephoria/lib/python2.7/site-packages/adminapi-1.4.3.0.1.1-py2.7.egg/cloud_utils/log_utils/__init__.py", line 351, in methdecor
    return func(*func_args, **func_kwargs)
  File "build/bdist.linux-x86_64/egg/nephoria/aws/ec2/ec2ops.py", line 2663, in create_snapshot_from_volume
    poll_interval=poll_interval, timeout=timeout, description=description)[0]
  File "/mnt/swathi/dev-swathi-EUCA-12910/97/envnephoria/lib/python2.7/site-packages/adminapi-1.4.3.0.1.1-py2.7.egg/cloud_utils/log_utils/__init__.py", line 351, in methdecor
    return func(*func_args, **func_kwargs)
  File "build/bdist.linux-x86_64/egg/nephoria/aws/ec2/ec2ops.py", line 2849, in create_snapshots
    delete_failed=delete_failed
  File "/mnt/swathi/dev-swathi-EUCA-12910/97/envnephoria/lib/python2.7/site-packages/adminapi-1.4.3.0.1.1-py2.7.egg/cloud_utils/log_utils/__init__.py", line 351, in methdecor
    return func(*func_args, **func_kwargs)
  File "build/bdist.linux-x86_64/egg/nephoria/aws/ec2/ec2ops.py", line 2953, in monitor_eusnaps_to_completed
    raise e
Exception: Snapshot:snap-a0e3a15a failed after Polling(2) ,Waited(0 sec), last reported (status:failed progress:0%)


[12-09 00:03:06][INFO][create_snapshots_all_vols_in_zone]:
----------------------------------------------------------------------------------
          - FAILURE -  TEST:"create_snapshots_all_vols_in_zone" COMPLETE
----------------------------------------------------------------------------------

[12-09 00:03:06][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      2        9         4        725
-----------------------------------------------

[12-09 00:03:06][INFO][create_vols_from_snap_in_same_zone]:
------------------------------------------------------------------------------------------------
 +--------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: create_vols_from_snap_in_same_zone                                      |
 | METHOD:create_vols_from_snap_in_same_zone, TEST DESCRIPTION:                               |
 |                                                                                            |
 | Description:                                                                               |
 | Attempts to create a volume from each snapshot contained in each zone's                    |
 | list of snapshots.                                                                         |
 | This test attempts to create volumes from snapshots who's original volume                  |
 | is also in this zone.                                                                      |
 |                                                                                            |
 | End on Failure:False                                                                       |
 | Passing ARGS:""                                                                            |
 | Running test method: "create_vols_from_snap_in_same_zone(timepergig=300, zonelist=None, )" |
 +--------------------------------------------------------------------------------------------+
------------------------------------------------------------------------------------------------

[12-09 00:03:06][DEBUG][create_vols_from_snap_in_same_zone]: snap-2c78fc2e, zone:one snap.eutest_volume_zone:one
[12-09 00:03:06][DEBUG][create_vols_from_snap_in_same_zone]: Adding snap to retlist:snap-2c78fc2e
[12-09 00:03:06][DEBUG][create_vols_from_snap_in_same_zone]: Creating volume from snap:snap-2c78fc2e
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1895)Starting method: create_volume(self, zone=one, size=0, eof=True, snapshot=Snapshot:snap-2c78fc2e, timeout=0, poll_interval=10, timepergig=300)
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:1916)Starting method: create_volumes(self, zone=one, size=0, count=1, mincount=1, eof=True, monitor_to_state=available, delay=0, snapshot=Snapshot:snap-2c78fc2e, timeout=0, poll_interval=10, timepergig=300)
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Sending create volume request, count:1
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: 1/1 requests for volume creation succeeded.
[12-09 00:03:06][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+----+----+-------------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS|AGE |SIZE|   SRC_SNAP  | MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+----+----+-------------+-----------+----+--------+
|vol-276ab668|  0  | creating |0.09| 1  |snap-2c78fc2e|None/(1024)|one |  None  |
+------------+-----+----------+----+----+-------------+-----------+----+--------+

[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2015)Starting method: monitor_created_euvolumes_to_state(self, volumes=[Volume:vol-276ab668], eof=True, mincount=1, state=available, poll_interval=10, deletefailed=True, size=1, timepergig=300)
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Monitoring 1 volumes for at least 1 to reach state:available
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Polling 1 volumes for status:"available"...
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-276ab668) State(creating), seconds elapsed: 0/300
[12-09 00:03:06][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:0, Waiting on 1 volumes to enter state:available-----
[12-09 00:03:16][DEBUG][EC2ops(admin:testrunner)]: Volume #0 (vol-276ab668) State(available), seconds elapsed: 10/300
[12-09 00:03:16][DEBUG][EC2ops(admin:testrunner)]: ----Time Elapsed:10, Waiting on 0 volumes to enter state:available-----
[12-09 00:03:17][INFO][EC2ops(admin:testrunner)]:
+------------+-----+----------+-----+----+-------------+-----------+----+--------+
|   VOL_ID   |ORDER|TESTSTATUS| AGE |SIZE|   SRC_SNAP  | MD5/(LEN) |ZONE|INSTANCE|
+------------+-----+----------+-----+----+-------------+-----------+----+--------+
|vol-276ab668|  0  |available |10.17| 1  |snap-2c78fc2e|None/(1024)|one |  None  |
+------------+-----+----------+-----+----+-------------+-----------+----+--------+

[12-09 00:03:17][INFO][create_vols_from_snap_in_same_zone]:
----------------------------------------------------------------------------------
         - SUCCESS -  TEST:"create_vols_from_snap_in_same_zone" COMPLETE
----------------------------------------------------------------------------------

[12-09 00:03:17][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      2        10        3        735
-----------------------------------------------

[12-09 00:03:17][INFO][attach_new_vols_from_snap_verify_md5]:
---------------------------------------------------------------------------------------------------------------
 +-----------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: attach_new_vols_from_snap_verify_md5                                                   |
 | METHOD:attach_new_vols_from_snap_verify_md5, TEST DESCRIPTION:                                            |
 |                                                                                                           |
 | Description:                                                                                              |
 | Attempts to attach volumes which were created from snapshots and are not                                  |
 | in use.                                                                                                   |
 | Iterates over test instances in zones for attaching the test volumes.                                     |
 | After verifying the volume is attached and reported as so by cloud and guest,                             |
 | this test will attempt to compare the md5 sum of the volume to the md5                                    |
 | contained in the snapshot which represents the md5 of the original volume.                                |
 | This test accepts a timepergig value which is used to guesstimate a                                       |
 | reasobale timeout while waiting for the md5 operation to be executed.                                     |
 |                                                                                                           |
 | End on Failure:False                                                                                      |
 | Passing ARGS:""                                                                                           |
 | Running test method: "attach_new_vols_from_snap_verify_md5(timepergig=480, timeout=480, zonelist=None, )" |
 +-----------------------------------------------------------------------------------------------------------+
---------------------------------------------------------------------------------------------------------------

[12-09 00:03:17][DEBUG][attach_new_vols_from_snap_verify_md5]: checking zone:one
[12-09 00:03:17][DEBUG][attach_new_vols_from_snap_verify_md5]: Checking volumes associated with snap:snap-2c78fc2e
[12-09 00:03:17][DEBUG][attach_new_vols_from_snap_verify_md5]: Checking volume:vol-276ab668 status:available
[12-09 00:03:17][DEBUG][i-da5b9630]: Attempting to attach volume:vol-276ab668 to instance:i-da5b9630 to dev:None
[12-09 00:03:17][DEBUG][i-da5b9630]: [root@10.111.31.105]# ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-09 00:03:27][WARNING][i-da5b9630]: CMD:"ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'", Error while opening channel:ls -1 /dev/ | grep '^sd\|^vd\|^xd\|^xvd'
[12-09 00:03:27][WARNING][i-da5b9630]: Attempting to reconnect and open channel again...
[12-09 00:03:27][DEBUG][i-da5b9630]: SSH connection attempt(1 of 3), host:'root@10.111.31.105', using ipv4:10.111.31.105, thru proxy:'None'
[12-09 00:03:27][DEBUG][i-da5b9630]: Failed to connect to 10.111.31.105, retry in 10 seconds. Err:Authentication failed.
[12-09 00:03:37][DEBUG][i-da5b9630]: SSH connection attempt(2 of 3), host:'root@10.111.31.105', using ipv4:10.111.31.105, thru proxy:'None'
[12-09 00:03:37][DEBUG][i-da5b9630]: Failed to connect to 10.111.31.105, retry in 10 seconds. Err:Authentication failed.
[12-09 00:03:47][DEBUG][i-da5b9630]: SSH connection attempt(3 of 3), host:'root@10.111.31.105', using ipv4:10.111.31.105, thru proxy:'None'
[12-09 00:03:47][DEBUG][i-da5b9630]: Failed to connect to 10.111.31.105, retry in 10 seconds. Err:Authentication failed.
[12-09 00:03:57][ERROR][attach_new_vols_from_snap_verify_md5]: Failed to attach volume:'vol-276ab668' to instance:'i-da5b9630'
Traceback (most recent call last):
  File "build/bdist.linux-x86_64/egg/nephoria/testcase_utils/cli_test_runner.py", line 263, in run
    ret = self.method(*args, **kwargs)
  File "nephoria/testcases/ec2/ebs/legacy_ebs_test_suite.py", line 720, in attach_new_vols_from_snap_verify_md5
    raise e
Exception: Failed to connect to "10.111.31.105", attempts:3. IPs tried:10.111.31.105


[12-09 00:03:57][INFO][attach_new_vols_from_snap_verify_md5]:
----------------------------------------------------------------------------------
        - FAILURE -  TEST:"attach_new_vols_from_snap_verify_md5" COMPLETE
----------------------------------------------------------------------------------

[12-09 00:03:57][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      3        10        2        775
-----------------------------------------------

[12-09 00:03:57][INFO][detach_all_volumes_from_stopped_instances_in_zones]:
-------------------------------------------------------------------------------------------------------------
 +---------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: detach_all_volumes_from_stopped_instances_in_zones                                   |
 | METHOD:detach_all_volumes_from_stopped_instances_in_zones, TEST DESCRIPTION:                            |
 |                                                                                                         |
 | Description:                                                                                            |
 | Attempts to detach volumes from instances while in the stopped state and                                |
 | verify volumes are detached, and upon instance start verify that both guest                             |
 | and cloud states are correct.                                                                           |
 |                                                                                                         |
 | End on Failure:False                                                                                    |
 | Passing ARGS:""                                                                                         |
 | Running test method: "detach_all_volumes_from_stopped_instances_in_zones(timeout=480, zonelist=None, )" |
 +---------------------------------------------------------------------------------------------------------+
-------------------------------------------------------------------------------------------------------------

[12-09 00:03:57][INFO][detach_all_volumes_from_stopped_instances_in_zones]:
----------------------------------------------------------------------------------
 - SUCCESS -  TEST:"detach_all_volumes_from_stopped_instances_in_zones" COMPLETE
----------------------------------------------------------------------------------

[12-09 00:03:57][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      3        11        1        775
-----------------------------------------------

[12-09 00:03:57][INFO][terminate_instances_in_zones_verify_volume_detach]:
------------------------------------------------------------------------------------------------------------
 +--------------------------------------------------------------------------------------------------------+
 | STARTING TESTUNIT: terminate_instances_in_zones_verify_volume_detach                                   |
 | METHOD:terminate_instances_in_zones_verify_volume_detach, TEST DESCRIPTION:                            |
 |                                                                                                        |
 | Description:                                                                                           |
 | Iterates over all instances in this testcase's zonelist attempts to                                    |
 | terminate the instances, and verify the attached volumes go to available                               |
 | after the instances are terminated.                                                                    |
 |                                                                                                        |
 | End on Failure:False                                                                                   |
 | Passing ARGS:""                                                                                        |
 | Running test method: "terminate_instances_in_zones_verify_volume_detach(timeout=480, zonelist=None, )" |
 +--------------------------------------------------------------------------------------------------------+
------------------------------------------------------------------------------------------------------------

[12-09 00:03:57][DEBUG][i-da5b9630]: Checking euinstance attached volumes states are in sync with clouds
[12-09 00:03:57][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-0147336f, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:03:57][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-0b3c3228, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:03:57][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-abf37b3d, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:03:57][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-, status=None, attached_instance=i-da5b9630, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630, has volume:vol-abf37b3d mapped at device:/dev/vdg
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630, has volume:vol-0b3c3228 mapped at device:/dev/vdf
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630, has volume:vol-0147336f mapped at device:/dev/vde
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630, has volume:vol-94c44f8a mapped at device:/dev/sda
[12-09 00:03:58][DEBUG][EC2ops(admin:testrunner)]: Beginning poll loop for instance Instance:i-da5b9630 to go to terminated
[12-09 00:03:58][DEBUG][EC2ops(admin:testrunner)]: Instance(i-da5b9630) State(terminated) time elapsed (0)
[12-09 00:03:58][DEBUG][EC2ops(admin:testrunner)]: Instance:i-da5b9630 is now in terminated
[12-09 00:03:58][DEBUG][i-da5b9630]: volume:vol-0147336f/in-use, from BDM, D.O.T.:False, waiting on status:available, elapsed:0/180
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630 terminated, vol-0147336f/available: volume entered expected state:available
[12-09 00:03:58][DEBUG][i-da5b9630]: volume:vol-abf37b3d/in-use, from BDM, D.O.T.:False, waiting on status:available, elapsed:0/180
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630 terminated, vol-abf37b3d/available: volume entered expected state:available
[12-09 00:03:58][DEBUG][i-da5b9630]: volume:vol-94c44f8a/in-use, from BDM, D.O.T.:True, waiting on status:deleted, elapsed:0/180
[12-09 00:03:58][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-94c44f8a, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-09 00:03:58][DEBUG][i-da5b9630]: volume:vol-0b3c3228/in-use, from BDM, D.O.T.:False, waiting on status:available, elapsed:0/180
[12-09 00:03:58][DEBUG][i-da5b9630]: i-da5b9630 terminated, vol-0b3c3228/available: volume entered expected state:available
[12-09 00:04:08][DEBUG][i-da5b9630]: volume:vol-94c44f8a/in-use, from BDM, D.O.T.:True, waiting on status:deleted, elapsed:10/180
[12-09 00:04:08][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-94c44f8a, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-09 00:04:18][DEBUG][i-da5b9630]: volume:vol-94c44f8a/deleting, from BDM, D.O.T.:True, waiting on status:deleted, elapsed:20/180
[12-09 00:04:18][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-94c44f8a, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=False)
[12-09 00:04:28][DEBUG][i-da5b9630]: volume:vol-94c44f8a/deleting, from BDM, D.O.T.:True, waiting on status:deleted, elapsed:30/180
[12-09 00:04:28][DEBUG][i-da5b9630]: i-da5b9630 terminated, vol-94c44f8a/deleted: volume entered expected state:deleted
[12-09 00:04:28][DEBUG][i-da5b9630]: Checking previously attached ENI status post instance terminate...
[12-09 00:04:28][INFO][terminate_instances_in_zones_verify_volume_detach]:
----------------------------------------------------------------------------------
  - SUCCESS -  TEST:"terminate_instances_in_zones_verify_volume_detach" COMPLETE
----------------------------------------------------------------------------------

[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]:
LATEST RESULTS:
-----------------------------------------------
  TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
-----------------------------------------------
    15      3        12        0        806
-----------------------------------------------

[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]: Printing pre-cleanup results:
[12-09 00:04:28][INFO][LegacyEbsTestSuite]: Test list results for testcase:LegacyEbsTestSuite
[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]: Creating TestUnit: "clean_method" with args:
[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]: Attempting to populate testunit:clean_method, with testcase.args...
[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]: Testunit keyword args:{}
[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]: Got method args:('self',)
[12-09 00:04:28][DEBUG][LegacyEbsTestSuite]: test unit total args:{}
[12-09 00:04:28][INFO][LegacyEbsTestSuite]:
--------------------------------------------------------------------------------------
 +----------------------------------------------------------------------------------+
 | STARTING TESTUNIT: clean_method                                                  |
 | METHOD:clean_method, TEST DESCRIPTION:                                           |
 |                                                                                  |
 | Definition:                                                                      |
 | Attempts to clean up test artifacts created during this test                     |
 |                                                                                  |
 | End on Failure:False                                                             |
 | Passing ARGS:""                                                                  |
 | Running test method: "clean_method()"                                            |
 +----------------------------------------------------------------------------------+
--------------------------------------------------------------------------------------

[12-09 00:04:28][DEBUG][EC2ops(admin:testrunner)]: Sending delete for volume: vol-0147336f
[12-09 00:04:28][DEBUG][EC2ops(admin:testrunner)]: Sent delete for volume: vol-0147336f, monitor to deleted state or failure
[12-09 00:04:28][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-0147336f, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:04:29][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0147336f in deleting sleeping:10, elapsed:0
[12-09 00:04:39][DEBUG][EC2ops(admin:testrunner)]: Sending delete for volume: vol-0b3c3228
[12-09 00:04:39][DEBUG][EC2ops(admin:testrunner)]: Sent delete for volume: vol-0b3c3228, monitor to deleted state or failure
[12-09 00:04:39][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-0b3c3228, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:04:39][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-0b3c3228 in deleting sleeping:10, elapsed:0
[12-09 00:04:49][DEBUG][EC2ops(admin:testrunner)]: Sending delete for volume: vol-abf37b3d
[12-09 00:04:49][DEBUG][EC2ops(admin:testrunner)]: Sent delete for volume: vol-abf37b3d, monitor to deleted state or failure
[12-09 00:04:49][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-abf37b3d, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:04:49][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-abf37b3d in deleting sleeping:10, elapsed:0
[12-09 00:04:59][DEBUG][EC2ops(admin:testrunner)]: Sending delete for volume: vol-276ab668
[12-09 00:04:59][DEBUG][EC2ops(admin:testrunner)]: Sent delete for volume: vol-276ab668, monitor to deleted state or failure
[12-09 00:04:59][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3813)Starting method: get_volumes(self, volume_id=vol-276ab668, status=None, attached_instance=None, attached_dev=None, snapid=None, zone=None, filters=None, minsize=1, maxsize=None, md5=None, eof=True)
[12-09 00:04:59][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-276ab668 in deleting sleeping:10, elapsed:0
[12-09 00:05:09][DEBUG][EC2ops(admin:testrunner)]: Volume no longer found
[12-09 00:05:10][DEBUG][EC2ops(admin:testrunner)]: Volume no longer found
[12-09 00:05:20][DEBUG][EC2ops(admin:testrunner)]: ---Waiting for:2 volumes to delete. Sleeping:10, elapsed:0/180---
[12-09 00:05:20][DEBUG][EC2ops(admin:testrunner)]: Volume no longer found
[12-09 00:05:30][DEBUG][EC2ops(admin:testrunner)]: ---Waiting for:1 volumes to delete. Sleeping:10, elapsed:0/180---
[12-09 00:05:30][DEBUG][EC2ops(admin:testrunner)]: Volume:vol-276ab668 in deleted
[12-09 00:05:40][DEBUG][EC2ops(admin:testrunner)]: ---Waiting for:1 volumes to delete. Sleeping:10, elapsed:20/180---
[12-09 00:05:40][DEBUG][EC2ops(admin:testrunner)]: Volume no longer found
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: ---Waiting for:0 volumes to delete. Sleeping:10, elapsed:20/180---
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:3044)Starting method: delete_snapshots(self, snapshots=[Snapshot:snap-2c78fc2e], valid_states=completed,failed, base_timeout=60, add_time_per_snap=10, wait_for_valid_state=120, poll_interval=10, eof=False)
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]:
--->(ec2ops.py:2995)Starting method: get_snapshots(self, snapid=snap-2c78fc2e, volume_id=None, volume_size=None, volume_md5=None, filters=None, maxcount=1, owner_id=None)
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Snapshot:snap-92bbc7b1 no longer found on system
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Checking snap:snap-92bbc7b1 for match...
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Checking snapshot:snap-2c78fc2e status:completed
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Waiting for remaining 1 snaps to delete...
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Snapshot not found, assuming it's already deleted:snap-2c78fc2e
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Snapshot:snap-2c78fc2e is deleted
[12-09 00:05:50][DEBUG][EC2ops(admin:testrunner)]: Sending delete for keypair: LegacyEbsTestSuite_1481241111
[12-09 00:05:50][INFO][LegacyEbsTestSuite]: Test list results for testcase:LegacyEbsTestSuite
[12-09 00:05:50][INFO][LegacyEbsTestSuite]:
-------------------------------------------------------------------------------------
 -----------------------------------------------------------------------------------
   TEST RESULTS FOR "LegacyEbsTestSuite"
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | create_vols_per_zone                                           |
   | TIME:      | 13                                                             |
   | TEST ARGS: | create_vols_per_zone(timepergig=300, snapshot=None,            |
   |            | volsperzone=1, zonelist=None, size=1, )                        |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | expand_volume_size                                             |
   | TIME:      | 41                                                             |
   | TEST ARGS: | expand_volume_size(volsperzone=1, zonelist=None, size=1, )     |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | create_test_instances_for_zones                                |
   | TIME:      | 48                                                             |
   | TEST ARGS: | create_test_instances_for_zones(username=root, count=1,        |
   |            | group=None, image=None, instance_password=None,                |
   |            | vmtype=c1.medium, keypair=None, zonelist=None, )               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | attach_all_avail_vols_to_instances_in_zones                    |
   | TIME:      | 35                                                             |
   | TEST ARGS: | attach_all_avail_vols_to_instances_in_zones(timeout=480,       |
   |            | zonelist=None, overwrite=True, )                               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | negative_delete_attached_volumes_in_zones                      |
   | TIME:      | 1                                                              |
   | TEST ARGS: | negative_delete_attached_volumes_in_zones(timeout=60,          |
   |            | zonelist=None, )                                               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | negative_attach_in_use_volume_in_zones                         |
   | TIME:      | 1                                                              |
   | TEST ARGS: | negative_attach_in_use_volume_in_zones(timeout=480,            |
   |            | zonelist=None, )                                               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | create_vols_per_zone                                           |
   | TIME:      | 10                                                             |
   | TEST ARGS: | create_vols_per_zone(timepergig=300, snapshot=None,            |
   |            | volsperzone=1, zonelist=None, size=1, )                        |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | attach_all_avail_vols_to_instances_in_zones                    |
   | TIME:      | 27                                                             |
   | TEST ARGS: | attach_all_avail_vols_to_instances_in_zones(timeout=480,       |
   |            | zonelist=None, overwrite=True, )                               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | reboot_instances_in_zone_verify_volumes                        |
   | TIME:      | 34                                                             |
   | TEST ARGS: | reboot_instances_in_zone_verify_volumes(waitconnect=30,        |
   |            | timeout=480, zonelist=None, )                                  |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | FAILED                                                         |
   | TEST NAME  | detach_volumes_in_zones                                        |
   | TIME:      | 483                                                            |
   | TEST ARGS: | detach_volumes_in_zones(volcount=1, timeout=480,               |
   |            | zonelist=None, eof=False, )                                    |
   | OUTPUT:    | ERROR:(Exception("                                             |
   |            | Could not detach Volume:vol-0147336ffrom                       |
   |            | instance:i-da5b9630,err:vol-0147336f:DETACH FAILED - Volume    |
   |            | status remained at:in-use, attach_data_status:detaching,       |
   |            | instance: i-da5b9630"))                                        |
 -----------------------------------------------------------------------------------
   | RESULT:    | FAILED                                                         |
   | TEST NAME  | create_snapshots_all_vols_in_zone                              |
   | TIME:      | 32                                                             |
   | TEST ARGS: | create_snapshots_all_vols_in_zone(volstate=all, zonelist=None, |
   |            | wait_on_progress=20, )                                         |
   | OUTPUT:    | ERROR:(Exception("Snapshot:snap-a0e3a15a failed after          |
   |            | Polling(2) ,Waited(0 sec), last reported (status:failed        |
   |            | progress:0%)"))                                                |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | create_vols_from_snap_in_same_zone                             |
   | TIME:      | 10                                                             |
   | TEST ARGS: | create_vols_from_snap_in_same_zone(timepergig=300,             |
   |            | zonelist=None, )                                               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | FAILED                                                         |
   | TEST NAME  | attach_new_vols_from_snap_verify_md5                           |
   | TIME:      | 40                                                             |
   | TEST ARGS: | attach_new_vols_from_snap_verify_md5(timepergig=480,           |
   |            | timeout=480, zonelist=None, )                                  |
   | OUTPUT:    | ERROR:(Exception("Failed to connect to "10.111.31.105",        |
   |            | attempts:3. IPs tried:10.111.31.105"))                         |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | detach_all_volumes_from_stopped_instances_in_zones             |
   | TIME:      | 0                                                              |
   | TEST ARGS: | detach_all_volumes_from_stopped_instances_in_zones(timeout=480 |
   |            | , zonelist=None, )                                             |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | terminate_instances_in_zones_verify_volume_detach              |
   | TIME:      | 31                                                             |
   | TEST ARGS: | terminate_instances_in_zones_verify_volume_detach(timeout=480, |
   |            | zonelist=None, )                                               |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------
   | RESULT:    | PASSED                                                         |
   | TEST NAME  | clean_method                                                   |
   | TIME:      | 81                                                             |
   | TEST ARGS: | clean_method()                                                 |
   | OUTPUT:    | None                                                           |
 -----------------------------------------------------------------------------------


   LATEST RESULTS:
   -----------------------------------------------
     TOTAL   FAILED   PASSED   NOT_RUN   ELAPSED
   -----------------------------------------------
       16      3        13        0        887
   -----------------------------------------------


 -----------------------------------------------------------------------------------
-------------------------------------------------------------------------------------

passed:13 failed:3 not_run:0 total:16
type ending with status: "1"
Build step 'Execute shell' marked build as failure
Connecting to monitoring.qa2
Trying to send metric to Graphite server : 10.111.1.52:2003, Metric name: BUILD_DURATION On queue : jenkins.jobs.eutester-load-hvm-image With value : 899
Trying to send metric to Graphite server : 10.111.1.52:2003, Metric name: BUILD_FAILED On queue : jenkins.jobs.eutester-load-hvm-image With value : 1
Finished: FAILURE
