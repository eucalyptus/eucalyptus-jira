{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"18917","self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917","key":"EUCA-4872","fields":{"issuetype":{"self":"https://eucalyptus.atlassian.net/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://eucalyptus.atlassian.net/secure/viewavatar?size=xsmall&avatarId=14803&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":14803},"timespent":null,"customfield_13100":null,"project":{"self":"https://eucalyptus.atlassian.net/rest/api/2/project/10000","id":"10000","key":"EUCA","name":"Eucalyptus","avatarUrls":{"48x48":"https://eucalyptus.atlassian.net/secure/projectavatar?pid=10000&avatarId=10011","24x24":"https://eucalyptus.atlassian.net/secure/projectavatar?size=small&pid=10000&avatarId=10011","16x16":"https://eucalyptus.atlassian.net/secure/projectavatar?size=xsmall&pid=10000&avatarId=10011","32x32":"https://eucalyptus.atlassian.net/secure/projectavatar?size=medium&pid=10000&avatarId=10011"}},"customfield_11000":null,"fixVersions":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/version/11305","id":"11305","name":"3.2.2","archived":false,"released":true,"releaseDate":"2013-04-01"}],"customfield_11001":null,"aggregatetimespent":null,"customfield_10310":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/customFieldOption/10209","value":"CentOS 6","id":"10209"}],"resolution":{"self":"https://eucalyptus.atlassian.net/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_10311":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/customFieldOption/10307","value":"Managed","id":"10307"}],"customfield_11401":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"customfield_11400":null,"customfield_10104":[],"customfield_10500":{"self":"https://eucalyptus.atlassian.net/rest/api/2/customFieldOption/10403","value":"Not Applicable","id":"10403"},"customfield_10302":null,"customfield_10105":{"self":"https://eucalyptus.atlassian.net/rest/api/2/customFieldOption/10101","value":"No","id":"10101"},"customfield_10303":null,"customfield_10700":null,"customfield_10304":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/version/11305","id":"11305","name":"3.2.2","archived":false,"released":true,"releaseDate":"2013-04-01"}],"customfield_10701":null,"customfield_10306":null,"customfield_10702":null,"customfield_10703":null,"customfield_10308":null,"customfield_10704":null,"resolutiondate":"2013-03-25T17:05:14.148-0500","customfield_10309":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/customFieldOption/10204","value":"KVM","id":"10204"}],"customfield_10705":null,"workratio":-1,"watches":{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/EUCA-4872/watchers","watchCount":9,"isWatching":false},"lastViewed":null,"created":"2013-02-01T00:19:02.384-0600","customfield_12000":null,"priority":{"self":"https://eucalyptus.atlassian.net/rest/api/2/priority/1","iconUrl":"https://eucalyptus.atlassian.net/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"customfield_12400":null,"customfield_12201":null,"customfield_10300":null,"customfield_12600":"{repository={count=1, dataType=repository}, json={\"cachedValue\":{\"errors\":[],\"summary\":{\"repository\":{\"overall\":{\"count\":1,\"lastUpdated\":\"2013-03-04T17:25:22.000-0600\",\"dataType\":\"repository\"},\"byInstanceType\":{\"github\":{\"count\":1,\"name\":\"GitHub\"}}}}},\"isStale\":true}}","customfield_10102":null,"labels":[],"customfield_10301":null,"customfield_11700":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"versions":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/version/10112","id":"10112","description":"Eucalyptus 3.2.0","name":"3.2.0","archived":false,"released":true,"releaseDate":"2012-12-19"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/version/11300","id":"11300","name":"3.2.1","archived":false,"released":true,"releaseDate":"2013-02-14"}],"customfield_11901":null,"issuelinks":[{"id":"15647","self":"https://eucalyptus.atlassian.net/rest/api/2/issueLink/15647","type":{"id":"10103","name":"Relates","inward":"relates to","outward":"relates to","self":"https://eucalyptus.atlassian.net/rest/api/2/issueLinkType/10103"},"inwardIssue":{"id":"19062","key":"DOC-720","self":"https://eucalyptus.atlassian.net/rest/api/2/issue/19062","fields":{"summary":"Attaching volumes to bfebs instance can cause fs mounted on backing vol to err or lockup","status":{"self":"https://eucalyptus.atlassian.net/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.","iconUrl":"https://eucalyptus.atlassian.net/images/icons/statuses/generic.png","name":"Closed","id":"6","statusCategory":{"self":"https://eucalyptus.atlassian.net/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://eucalyptus.atlassian.net/rest/api/2/priority/3","iconUrl":"https://eucalyptus.atlassian.net/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://eucalyptus.atlassian.net/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://eucalyptus.atlassian.net/secure/viewavatar?size=xsmall&avatarId=14803&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":14803}}}}],"assignee":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"updated":"2014-06-16T16:08:13.463-0500","status":{"self":"https://eucalyptus.atlassian.net/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.","iconUrl":"https://eucalyptus.atlassian.net/images/icons/statuses/generic.png","name":"Closed","id":"6","statusCategory":{"self":"https://eucalyptus.atlassian.net/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/component/10010","id":"10010","name":"Node Controller"}],"timeoriginalestimate":null,"description":"This system is using VNX and is configured for mpath. \r\nNot sure if this is something related to the VNX or the bfebs image yet (ie virtio, acpi hotplug, etc.). \r\n\r\nWas leaning more towards the image since this test passed not too long ago. However, today similar tests were run using the same image against another SC/SAN and not seen this behavior. Depending on how the FS is being accessed it may go unnoticed in basic tests. \r\n\r\nNote: '/' is mounted with errors option 'remount-ro', this could be changed to 'panic' to expedite the test? \r\nFor example:\r\ninstanceprompt/# mount\r\n/dev/vda1 on / type ext4 (rw,errors=remount-ro)#could be errors=panic\r\n\r\n\r\n\r\nTo reproduce:\r\n-launch bfebs instance using VNX for backing vol\r\n-attach volume(s) to instance (tail /var/log/messages or dmesg on instance) to look for faults or wait for command and/or connection to begin failing. \r\n\r\n\r\n\r\n{code}\r\n960.104175] INFO: task jbd2/vda1-8:308 blocked for more than 120 seconds.\r\n[  960.104185] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  960.104194] jbd2/vda1-8   D 00004da9     0   308      2 0x00000000\r\n[  960.104200]  dddebde4 00000046 df884000 00004da9 00000000 c088e5c0 dddac264 c088e5c0\r\n[  960.104209]  83302583 000000b3 c088e5c0 c088e5c0 dddac264 c088e5c0 c088e5c0 ddc1ca80\r\n[  960.104217]  00000000 000000b3 dddabfc0 dddabfc0 c16095c0 dddebe2c dddebdf4 c05b04e1\r\n[  960.104227] Call Trace:\r\n[  960.104235]  [<c05b04e1>] io_schedule+0x61/0xa0\r\n[  960.104239]  [<c01d441d>] sync_page+0x3d/0x50\r\n[  960.104242]  [<c05b0c9d>] __wait_on_bit+0x4d/0x70\r\n[  960.104244]  [<c01d43e0>] ? sync_page+0x0/0x50\r\n[  960.104246]  [<c01d4641>] wait_on_page_bit+0x91/0xa0\r\n[  960.104249]  [<c0170990>] ? wake_bit_function+0x0/0x50\r\n[  960.104252]  [<c01d48d1>] wait_on_page_writeback_range+0xa1/0x110\r\n[  960.104256]  [<c023be3e>] ? bio_alloc_bioset+0x2e/0xf0\r\n[  960.104260]  [<c02d9027>] ? jbd2_journal_write_metadata_buffer+0x277/0x370\r\n[  960.104263]  [<c01d499a>] filemap_fdatawait+0x5a/0x70\r\n[  960.104267]  [<c02d129e>] jbd2_journal_commit_transaction+0x51e/0x1030\r\n[  960.104270]  [<c0150c08>] ? dequeue_task_fair+0x68/0x70\r\n[  960.104273]  [<c0131008>] ? default_spin_lock_flags+0x8/0x10\r\n[  960.104278]  [<c01649b8>] ? try_to_del_timer_sync+0x68/0xb0\r\n[  960.104280]  [<c02d7a85>] kjournald2+0x95/0x1c0\r\n[  960.104283]  [<c0170940>] ? autoremove_wake_function+0x0/0x50\r\n[  960.104285]  [<c02d79f0>] ? kjournald2+0x0/0x1c0\r\n[  960.104287]  [<c01706b4>] kthread+0x74/0x80\r\n[  960.104290]  [<c0170640>] ? kthread+0x0/0x80\r\n[  960.104293]  [<c010a447>] kernel_thread_helper+0x7/0x10\r\n\r\n{code}\r\n\r\n\r\n\r\nNC snippet from /var/log/messages DM not happy...\r\n\r\n\r\n{code}\r\nJan 31 21:56:27 CENTOS-x86-64 kernel: device-mapper: multipath: Failing path 8:48.\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdc: add path (uevent)\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdc: alua not supported\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sde: add path (uevent)\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sde: alua not supported\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdf: add path (uevent)\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdf: spurious uevent, path already in pathvec\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdb: add path (uevent)\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdb: spurious uevent, path already in pathvec\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdd: add path (uevent)\r\nJan 31 21:56:28 CENTOS-x86-64 multipathd: sdd: spurious uevent, path already in pathvec\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291328\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291328\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291440\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291440\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291392\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291440\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 6291448\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 2048\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 24\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 24\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 24\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 24\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 120\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 120\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 120\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 120\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 24\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 24\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 120\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 120\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 16\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 16\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 64\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 256\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 16\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 16\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 16\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 8\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 128\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 4096\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 56\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:28 CENTOS-x86-64 kernel: end_request: I/O error, dev dm-1, sector 0\r\nJan 31 21:56:31 CENTOS-x86-64 multipathd: mpatho: sdf - directio checker reports path is up\r\nJan 31 21:56:31 CENTOS-x86-64 multipathd: 8:80: reinstated\r\nJan 31 21:56:31 CENTOS-x86-64 multipathd: mpatho: remaining active paths: 1\r\nJan 31 21:56:31 CENTOS-x86-64 kernel: sd 9:0:0:1: alua: port group 02 state A supports tolUsNA\r\nJan 31 21:56:31 CENTOS-x86-64 kernel: device-mapper: multipath: Failing path 8:80.\r\nJan 31 21:56:31 CENTOS-x86-64 multipathd: mpatho: failed to update map : Invalid argument\r\nJan 31 21:56:31 CENTOS-x86-64 kernel: device-mapper: table: 253:2: sdd too small for target: start=0, len=6291456, dev_size=2097152\r\nJan 31 21:56:31 CENTOS-x86-64 multipathd: 8:80: mark as failed\r\nJan 31 21:56:31 CENTOS-x86-64 multipathd: mpatho: remaining active paths: 0\r\nJan 31 21:56:33 CENTOS-x86-64 multipathd: mpatho: sdd - directio checker reports path is up\r\nJan 31 21:56:33 CENTOS-x86-64 multipathd: DM message failed [reinstate_path 8:48#012]\r\nJan 31 21:56:33 CENTOS-x86-64 multipathd: 8:48: reinstate failed\r\nJan 31 21:56:33 CENTOS-x86-64 multipathd: mpatho: failed to update map : Invalid argument\r\nJan 31 21:56:33 CENTOS-x86-64 kernel: device-mapper: table: 253:2: sdf too small for target: start=0, len=6291456, dev_size=2097152\r\nJan 31 21:56:37 CENTOS-x86-64 multipathd: mpatho: sdf - directio checker reports path is up\r\nJan 31 21:56:37 CENTOS-x86-64 multipathd: DM message failed [reinstate_path 8:80#012]\r\nJan 31 21:56:37 CENTOS-x86-64 multipathd: 8:80: reinstate failed\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: INFO: task qemu-kvm:3833 blocked for more than 120 seconds.\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: qemu-kvm      D 0000000000000002     0  3833      1 0x00000080\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: ffff8801e705faf8 0000000000000082 0000000000000000 ffffea000552c900\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: ffff880234cfe908 ffff8801e705fc68 ffffffff811ae020 0000000000000286\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: ffff88017bdc7af8 ffff8801e705ffd8 000000000000fb88 ffff88017bdc7af8\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: Call Trace:\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811ae020>] ? blkdev_get_block+0x0/0x70\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff8110fda0>] ? sync_page+0x0/0x50\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff814ea3e3>] io_schedule+0x73/0xc0\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff8110fddd>] sync_page+0x3d/0x50\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff814ead9f>] __wait_on_bit+0x5f/0x90\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81110013>] wait_on_page_bit+0x73/0x80\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff810909d0>] ? wake_bit_function+0x0/0x50\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81126425>] ? pagevec_lookup_tag+0x25/0x40\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff8111048b>] wait_on_page_writeback_range+0xfb/0x190\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811255a4>] ? generic_writepages+0x24/0x30\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811255d1>] ? do_writepages+0x21/0x40\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811105db>] ? __filemap_fdatawrite_range+0x5b/0x60\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81110658>] filemap_write_and_wait_range+0x78/0x90\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811a4dfe>] vfs_fsync_range+0x7e/0xe0\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811a4eab>] generic_write_sync+0x4b/0x50\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff811b05ce>] blkdev_aio_write+0xce/0x130\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff8117628a>] do_sync_write+0xfa/0x140\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81082442>] ? send_signal+0x42/0x80\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81090990>] ? autoremove_wake_function+0x0/0x40\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81082856>] ? group_send_sig_info+0x56/0x70\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff810828af>] ? kill_pid_info+0x3f/0x60\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff8120ca26>] ? security_file_permission+0x16/0x20\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81176588>] vfs_write+0xb8/0x1a0\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff81176f42>] sys_pwrite64+0x82/0xa0\r\nJan 31 21:59:11 CENTOS-x86-64 kernel: [<ffffffff8100b072>] system_call_fastpath+0x16/0x1b\r\nJan 31 21:59:27 CENTOS-x86-64 udevd[1041]: worker [2787] unexpectedly returned with status 0x0100\r\nJan 31 21:59:27 CENTOS-x86-64 udevd[1041]: worker [2787] failed while handling '/devices/virtual/block/dm-3'\r\nJan 31 21:59:27 CENTOS-x86-64 udevd[1041]: worker [2788] unexpectedly returned with status 0x0100\r\nJan 31 21:59:27 CENTOS-x86-64 udevd[1041]: worker [2788] failed while handling '/devices/virtual/block/dm-4'\r\nJan 31 21:59:27 CENTOS-x86-64 udevd[1041]: worker [2789] unexpectedly returned with status 0x0100\r\nJan 31 21:59:27 CENTOS-x86-64 udevd[1041]: worker [2789] failed while handling '/devices/virtual/block/dm-5'\r\nJan 31 21:59:31 CENTOS-x86-64 udevd[1041]: worker [32050] unexpectedly returned with status 0x0100\r\nJan 31 21:59:31 CENTOS-x86-64 udevd[1041]: worker [32050] failed while handling '/devices/virtual/block/dm-2'\r\n{code}","customfield_13000":null,"customfield_13200":null,"customfield_11300":null,"customfield_11500":null,"timetracking":{},"customfield_10401":null,"customfield_10005":null,"customfield_10600":null,"customfield_12900":null,"aggregatetimeestimate":null,"attachment":[],"summary":"Attaching volumes to bfebs instance can cause fs mounted on backing vol to err or lockup","creator":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"reporter":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"customfield_12100":"0|i01txj:","customfield_10000":"3_*:*_1_*:*_2415394339_*|*_10001_*:*_1_*:*_16189_*|*_10000_*:*_1_*:*_304521442_*|*_10006_*:*_1_*:*_0_*|*_10003_*:*_1_*:*_1808299039_*|*_10007_*:*_1_*:*_21340823","aggregateprogress":{"progress":0,"total":0},"customfield_10001":"2013-02-04T11:18:29.849-0600","customfield_10002":null,"customfield_10200":null,"customfield_10003":null,"customfield_12500":null,"customfield_10400":null,"customfield_10116":null,"customfield_11600":["awithrow(awithrow)","mclark(mclark)","tjcramer(tjcramer)","zhill(zhill)"],"customfield_10117":null,"environment":"192.168.51.69\tCENTOS\t6.3\t64\tREPO\t[CLC WS]\r\n192.168.51.70\tCENTOS\t6.3\t64\tREPO\t[CC00 SC00]\r\n192.168.51.72\tCENTOS\t6.3\t64\tREPO\t[NC00]\r\n192.168.51.73\tCENTOS\t6.3\t64\tREPO\t[NC00]","customfield_11800":null,"customfield_11802":null,"duedate":null,"customfield_11805":null,"progress":{"progress":0,"total":0},"customfield_11806":null,"comment":{"comments":[{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39248","id":"39248","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"body":"Eventually left in this state the NC and/or libvirt hangs and does not recover. This also leaves/left an instance(s) running, virsh destroy is failing to remove the domain. Error looks like may be around killing the network process?\r\n\r\nGDB bt for hung NC...\r\n\r\n{code}\r\n0x00007f6e55c27a63 in __poll (fds=<value optimized out>, nfds=<value optimized out>, timeout=<value optimized out>) at ../sysdeps/unix/sysv/linux/poll.c:87\r\n87\t  int result = INLINE_SYSCALL (poll, 3, CHECK_N (fds, nfds), nfds, timeout);\r\nMissing separate debuginfos, use: debuginfo-install audit-libs-2.2-2.el6.x86_64 augeas-libs-0.9.0-4.el6.x86_64 avahi-libs-0.6.25-11.el6.x86_64 axis2c-1.6.0-0.5.el6.x86_64 cyrus-sasl-lib-2.1.23-13.el6_3.1.x86_64 dbus-libs-1.2.24-7.el6_3.x86_64 device-mapper-libs-1.02.74-10.el6_3.3.x86_64 eucalyptus-gl-3.2.1-0.0.500.20130129gitcb9e24bc.el6.x86_64 eucalyptus-nc-3.2.1-0.0.500.20130129gitcb9e24bc.el6.x86_64 gnutls-2.8.5-4.el6_2.2.x86_64 keyutils-libs-1.4-4.el6.x86_64 krb5-libs-1.9-33.el6_3.3.x86_64 libcap-ng-0.6.4-3.el6_0.1.x86_64 libcom_err-1.41.12-12.el6.x86_64 libcurl-7.19.7-26.el6_2.4.x86_64 libgcc-4.4.6-4.el6.x86_64 libgcrypt-1.4.5-9.el6_2.2.x86_64 libgpg-error-1.7-4.el6.x86_64 libidn-1.18-2.el6.x86_64 libnl-1.1-14.el6.x86_64 libpcap-1.0.0-6.20091201git117cb5.el6.x86_64 libsepol-2.0.41-4.el6.x86_64 libssh2-1.2.2-11.el6_3.x86_64 libtasn1-2.3-3.el6_2.1.x86_64 libudev-147-2.42.el6.x86_64 libuuid-2.17.2-12.7.el6_3.x86_64 libvirt-client-0.9.10-21.el6_3.8.x86_64 libxml2-2.7.6-8.el6_3.4.x86_64 libxslt-1.1.26-2.el6_3.1.x86_64 netcf-libs-0.1.9-2.el6.x86_64 nspr-4.9.1-2.el6_3.x86_64 nss-3.13.5-1.el6_3.x86_64 nss-softokn-freebl-3.12.9-11.el6.x86_64 nss-util-3.13.5-1.el6_3.x86_64 numactl-2.0.7-3.el6.x86_64 openssl-1.0.0-25.el6_3.1.x86_64 rampartc-1.3.0-0.5.el6.x86_64 yajl-1.0.7-3.el6.x86_64\r\n(gdb) bt\r\n#0  0x00007f6e55c27a63 in __poll (fds=<value optimized out>, nfds=<value optimized out>, timeout=<value optimized out>) at ../sysdeps/unix/sysv/linux/poll.c:87\r\n#1  0x00007f6e4f1985f1 in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#2  0x00007f6e4f198f79 in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#3  0x00007f6e4f199557 in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#4  0x00007f6e4f196060 in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#5  0x00007f6e4f1778ce in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#6  0x00007f6e4f17794c in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#7  0x00007f6e4f179e76 in ?? () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#8  0x00007f6e4f15a590 in virDomainDetachDevice () from /usr/lib64/axis2c/lib/libvirt.so.0\r\n#9  0x00007f6e4f98636a in ?? () from /usr/lib64/axis2c/services/EucalyptusNC/libEucalyptusNC.so\r\n#10 0x00007f6e4f988f3c in find_and_terminate_instance () from /usr/lib64/axis2c/services/EucalyptusNC/libEucalyptusNC.so\r\n#11 0x00007f6e4f9899ca in ?? () from /usr/lib64/axis2c/services/EucalyptusNC/libEucalyptusNC.so\r\n#12 0x00007f6e4f9750b4 in ncTerminateInstanceMarshal () from /usr/lib64/axis2c/services/EucalyptusNC/libEucalyptusNC.so\r\n#13 0x00007f6e4f970dd1 in axis2_svc_skel_EucalyptusNC_invoke () from /usr/lib64/axis2c/services/EucalyptusNC/libEucalyptusNC.so\r\n#14 0x00007f6e5508cc35 in ?? () from /usr/lib64/libaxis2_engine.so.0\r\n#15 0x00007f6e5508c7a1 in ?? () from /usr/lib64/libaxis2_engine.so.0\r\n#16 0x00007f6e55082d16 in axis2_engine_receive () from /usr/lib64/libaxis2_engine.so.0\r\n#17 0x00007f6e552da367 in axis2_http_transport_utils_process_http_post_request () from /usr/lib64/httpd/modules/libmod_axis2.so\r\n#18 0x00007f6e552d5d3c in axis2_apache2_worker_process_request () from /usr/lib64/httpd/modules/libmod_axis2.so\r\n#19 0x00007f6e552d3f7c in ?? () from /usr/lib64/httpd/modules/libmod_axis2.so\r\n#20 0x00007f6e57638b00 in ap_run_handler (r=0x7f6e57f30428) at /usr/src/debug/httpd-2.2.15/server/config.c:158\r\n#21 0x00007f6e5763c3be in ap_invoke_handler (r=0x7f6e57f30428) at /usr/src/debug/httpd-2.2.15/server/config.c:376\r\n#22 0x00007f6e57647a30 in ap_process_request (r=0x7f6e57f30428) at /usr/src/debug/httpd-2.2.15/modules/http/http_request.c:282\r\n#23 0x00007f6e576448f8 in ap_process_http_connection (c=0x7f6e57e50918) at /usr/src/debug/httpd-2.2.15/modules/http/http_core.c:190\r\n#24 0x00007f6e57640608 in ap_run_process_connection (c=0x7f6e57e50918) at /usr/src/debug/httpd-2.2.15/server/connection.c:43\r\n#25 0x00007f6e5764c807 in child_main (child_num_arg=<value optimized out>) at /usr/src/debug/httpd-2.2.15/server/mpm/prefork/prefork.c:667\r\n#26 0x00007f6e5764cb1a in make_child (s=0x7f6e57ddd860, slot=0) at /usr/src/debug/httpd-2.2.15/server/mpm/prefork/prefork.c:763\r\n#27 0x00007f6e5764d79c in perform_idle_server_maintenance (_pconf=<value optimized out>, plog=<value optimized out>, s=<value optimized out>)\r\n    at /usr/src/debug/httpd-2.2.15/server/mpm/prefork/prefork.c:898\r\n#28 ap_mpm_run (_pconf=<value optimized out>, plog=<value optimized out>, s=<value optimized out>) at /usr/src/debug/httpd-2.2.15/server/mpm/prefork/prefork.c:1102\r\n#29 0x00007f6e57624900 in main (argc=3, argv=0x7fffb0102408) at /usr/src/debug/httpd-2.2.15/server/main.c:760\r\n\r\n\r\n------------------------------------------------------------------------\r\n\r\nlibvirt error when attempting to destroy orphaned domain/instance...\r\n\r\n013-02-01 07:46:59.967+0000: 4182: warning : qemuProcessKill:3742 : Timed out waiting after SIGTERM to process 747, sending SIGKILL\r\n2013-02-01 07:47:01.368+0000: 4182: warning : qemuProcessKill:3765 : Timed out waiting after SIGKILL to process 747\r\n\r\n\r\n\r\n qemuProcessKill(struct qemud_driver *driver,\r\n>                  virDomainObjPtr vm, unsigned int flags)\r\n>  {\r\n.......\r\n>  \r\n> -    /* This loop sends SIGTERM (or SIGKILL if flags has\r\n> -     * VIR_QEMU_PROCESS_KILL_FORCE and VIR_QEMU_PROCESS_KILL_NOWAIT),\r\n> -     * then waits a few iterations (10 seconds) to see if it dies. If\r\n> -     * the qemu process still hasn't exited, and\r\n> -     * VIR_QEMU_PROCESS_KILL_FORCE is requested, a SIGKILL will then\r\n> -     * be sent, and qemuProcessKill will wait up to 5 seconds more for\r\n> -     * the process to exit before returning.  Note that the FORCE mode\r\n> -     * could result in lost data in the guest, so it should only be\r\n> -     * used if the guest is hung and can't be destroyed in any other\r\n> -     * manner.\r\n> -     */\r\n.....","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-01T01:55:52.470-0600","updated":"2013-02-01T01:56:29.188-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39262","id":"39262","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"body":"Also verified this against a non-mpath vnx system. \nIt looks like the first volume attached is trying to re-use the same device as the bfebs backing vol for the newly attached vol's iscsi connection? \n-------\n>>Launch original instance...\n2013-02-01 21:34:52 DEBUG 000010712 connect_iscsi_target     iscsi.c:125                       | connect script returned: 0, stdout: '/dev/sdd', stderr: \n-------\n>>Attach first volume:\n2013-02-01 21:38:19 DEBUG 000010605 connect_iscsi_target     iscsi.c:125                       | connect script returned: 0, stdout: '/dev/sdd', stderr: \n-------\n>>Attach second volume:\n2013-02-01 21:52:34 DEBUG 000010605 connect_iscsi_target     iscsi.c:125                       | connect script returned: 0, stdout: '/dev/sde', stderr: \n\n\nThanks,\n-M","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-02T00:10:16.622-0600","updated":"2013-02-02T00:10:16.622-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39305","id":"39305","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"body":"[~mclark], it looks like you've marked this as a blocker for 3.2.1; setting target as 3.2.1. \n\n[~dmitrii], is this something for you to take a look at?  It would be good to review and assign in advance of a formal bug scrub.  \n\nThanks.\n","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"created":"2013-02-04T11:18:29.849-0600","updated":"2013-02-04T11:18:29.849-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39311","id":"39311","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"body":"Looking at this now on the system Matt was using: 51.69,70,72,73","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-04T12:54:23.773-0600","updated":"2013-02-04T12:54:23.773-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39312","id":"39312","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"body":"It looks like there might be an issue with the VNX code not exporting the luns properly. I'm seeing luns exposed to both NCs that should only be on one. I'm looking into it now.\n\nWe may just now be seeing this because we are testing with multiple NCs instead of just one. Matt, do the GA runs normally use multiple NCs or just a single NC?","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-04T13:29:32.344-0600","updated":"2013-02-04T13:29:32.344-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39326","id":"39326","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"body":"Ok, I have confirmed the problem is both EMC specific and caused by how the SC manages access control to the LUNs on the VNX. There are no required changes on the NC or CLC to resolve the issue. This condition will only be seen on bfebs instances where the cluster has more than 1 NC.\n\nThe problem is that the sequence of operations from the CLC to the SC for running an EBS instance are:\n1. Create volume\n2. Export/Attach volume to *all* nodes in the cluster\n3. Run instance.\n\nStep 2 is the problem for the SC with EMC due to a design assumption being incorrect. The message that the CLC sends the SC for step 2 is attachVolume(volumeId, List_of_iqns). The assumption was that the list of iqns were all for the same host, which is not true.\n\nUp to this point everything is fine. The problem occurs when attaching a 2nd volume to the instance and only when the cluster has more than one NC.\n\nWhen creating the storagegroup for the rootfs volume for the ebs instance the SC generates a name for the group by using the hash of the sorted list of iqns. It then creates the group and adds the host iqns to that group. To give them access to the LUN, it adds the LUN to the group as well.\n\nThe problem occurs when a second attach request is received by SC. In this request there is only a single IQN, that of the NC to attach to, and therefore the hash for the storagegroup name is different than the has for the rootfs.\n\nThe SC creates a new group since the name is different, and then adds the iqn/host to that group. It then adds the LUN to be attached to that group and returns.\n\nThe problem is that when the host is added to the storagegroup, the EMC automatically *removes* it from all other groups (specifically, the one for the rootfs). So, the EMC just shut off access to the rootfs for the ebs-instance.\n\nWhen the NC does a iscsi session rescan either the same LUN is re-used (i.e. LUN 1 on the VNX is no longer the rootfs lun but the 2nd lun attached) or a new lun appears and the old one is removed. Either way, the VM does not have a rootfs anymore.\n\nTo verify this behavior I ran the basic sequence on the VNX without any Eucalyptus envolvement.\nHere is a reproduction of the problem without using eucalyptus at all:\n\n#Create a storage group 1 (to simulate the group that would contain all nodes in the cluster)\n$ navi.sh storagegroup -create -gname testgroup1\n\n#Create a 2nd storage group (to simulate the group that would contain just the single host)\n$ navi.sh storagegroup -create -gname testgroup2\n\n#Add a host to first group. You can ignore the warning.\n$ navi.sh storagegroup -gname testgroup1 -connecthost -host `cat my_iqn`\n\nWARNING: Changing configuration options may cause the array to stop functioning\ncorrectly. Failover-related Initiator settings for a single host MUST BE CONSISTENT\nfor all paths from the host to the storage system. Please verify after reconnect.\nConnect host iqn.1994-05.com.redhat:794f43a6a040 to storage group testgroup1  (y/n)? y\n\n#Show that it worked\n$ navi.sh storagegroup -list -gname testgroup1\n\nStorage Group Name:    testgroup1\nStorage Group UID:     08:73:63:91:28:6F:E2:11:BE:54:E7:9F:89:20:4D:1E\nHBA/SP Pairs:\n\n  HBA UID                                          SP Name     SPPort\n  -------                                          -------     ------ \n  iqn.1994-05.com.redhat:794f43a6a040               SP A         6\nShareable:             YES\n\n#Now, add the same host to the 2nd group, as is done on a vol attach.\n$ navi.sh storagegroup -gname testgroup2 -connecthost -host `cat my_iqn`\n\nWARNING: Changing configuration options may cause the array to stop functioning\ncorrectly. Failover-related Initiator settings for a single host MUST BE CONSISTENT\nfor all paths from the host to the storage system. Please verify after reconnect.\nConnect host iqn.1994-05.com.redhat:794f43a6a040 to storage group testgroup2\nWarning: This host is currently mapped to storage group testgroup1.\nDo you still want to map it to storage group testgroup2?  (y/n)? y\n\n#What did that do to group 1? Removed the host silently.\n$ navi.sh storagegroup -list -gname testgroup1\n\nStorage Group Name:    testgroup1\nStorage Group UID:     08:73:63:91:28:6F:E2:11:BE:54:E7:9F:89:20:4D:1E\nShareable:             YES\n\n#It is now *only* in group 2\n[zhill@centos6vm euca4872_investigation]$ navi.sh storagegroup -list -gname testgroup2\n\nStorage Group Name:    testgroup2\nStorage Group UID:     09:73:63:91:28:6F:E2:11:BE:54:E7:9F:89:20:4D:1E\nHBA/SP Pairs:\n\n  HBA UID                                          SP Name     SPPort\n  -------                                          -------     ------ \n  iqn.1994-05.com.redhat:794f43a6a040               SP A         6\nShareable:             YES\n\n\nThe solution to this is to re-write how the EMC provider code on the SC allocates and names storagegroups such that a group is unique to a host and a host will never be added to multiple groups.\n\nI have started work on this already and will update with progress.\n","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-04T18:39:44.767-0600","updated":"2013-02-04T18:39:44.767-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39327","id":"39327","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"body":"Reproducible with 2 NCs and bfebs instance.","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-04T18:50:04.645-0600","updated":"2013-02-04T18:50:04.645-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39425","id":"39425","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"body":"Changing Target version to 3.2.2.  This will not block 3.2.1 release; the subset of customers who may be affected by this issue will need to wait to upgrade until 3.2.2 when this is addressed. ","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T12:00:13.910-0600","updated":"2013-02-07T12:00:13.910-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39434","id":"39434","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=tjcramer","name":"tjcramer","key":"tjcramer","accountId":"557058:e076cf45-80ca-4b67-befb-7b0c21f32739","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue"},"displayName":"Tim Cramer","active":true,"timeZone":"America/Los_Angeles"},"body":"To be clear.  The only issue is currently with:\n  - BFEBS image\n  - 2 or more NC's\n  - BFEBS instance attaches a volume\nwhich can cause data inconsistency.  \n\nOnly short term workaround would be to create a cluster with only 1 NC and push BFEBS instances to that AZ/cluster.  ","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=tjcramer","name":"tjcramer","key":"tjcramer","accountId":"557058:e076cf45-80ca-4b67-befb-7b0c21f32739","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/65e7a5c18fc469f1978f6367993c3bf0?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dtjcramer%26avatarId%3D10401%26noRedirect%3Dtrue"},"displayName":"Tim Cramer","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T13:21:39.271-0600","updated":"2013-02-07T13:21:39.271-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/39612","id":"39612","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"body":"Fix status update:\n\nI have implemented a new export protocol for the EMC provider.\n\nThe old algorithm was (this is pseudocode):\n{code}\nexport(volumeId, List iqns) {\n group_name = hash(sort(iqns))\n emc_group = emc.get_group(group_name)\n if(!emc_group.contains(iqns)){\n   emc_group.add(iqns)\n }\n if(!emc_group.contains(volumeId)) {\n  lun_id = emc_group.add(volumeId)\n }\n return lun_id\n}\n{code}\n\nThe new algorithm is:\n{code}\nexport(volumeId, List iqns) {\n foreach(iqn in iqns) {\n   group_name = compute_groupname(iqn)\n   emc_group = emc.get_group(group_name)\n   if(!emc_group.contains(iqn)){\n    emc_group.add(iqn)\n   }\n   if(!emc_group.contains(volumeId)) {\n    lunmap.add(iqn -> emc_group.add(volumeId))\n   }\n }\n return lunmap\n}\n{code}\n\nThere is also a bunch of error detection and rollback code to handle the case where some groups have the lun added, but not all and how to restore the export state to the state before export() was called. Rollback is important because in cases where the CLC calls export(volumeId, all_cluster_iqns) and the calls export(volumeId, exact_nc_iqn) the SC is expected not to modify the export for the 'exact_nx_iqn' since it is assumed that there is a data connection to that NC that must not be broken.\n\nThis changes the device string that the SC returns to the CLC from V1:\n<chapuser>,<authmethod>,<lun_id>,<password>,<dest IP>,<dest IQN>\nTo V2:\n<chapuser>,<authmethod>,<lun_mapping_list>,<password>,<dest IP>,<dest IQN>\n\nwhere lun_mapping_list='iqn1=lun1|iqn2=lun2|...|iqnN=lunN' where N is the number of NCs that the CLC requested the volume be exported to.\n\nWe have added a step in the CC to filter these mappings out and convert the string from V2 format to V1 format so that none of the endpoints have to be changed (NC or VMWareBroker) to support the new method. The CC selects the IQN=LUN entry for the NC that it is going to run the instance on when doing BfEBS and replaces the entire lun map in the string with just the lunID from that single entry.\n\nWith Dmitrii's help the CC has been modified and the SC as well. I am able to run BfEBS instances as well as attach/detach volumes to running instance-store instances. Next I will test attachments to BfEBS instances and then start negative testing to ensure that things like rollback and various cleanup mechanisms work properly.\n","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-13T11:48:18.104-0600","updated":"2013-02-13T11:48:18.104-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/41003","id":"41003","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"body":"[~zhill], can you give an update on this one?  You mentioned that you had some testing to do still.  Are you on track to have your work done by 3/8?  ","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"created":"2013-03-01T17:22:31.099-0600","updated":"2013-03-01T17:22:31.099-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/41049","id":"41049","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"body":"Committed to maint/3.2/testing. Commit id: aacbc8e23cd8a7f843f4688acb174597d019b839\r\n\r\n\r\nTesting:\r\nRan the main eucalyptus-3.2 test on an EMC configuration successfully and did a bunch of manual testing on setup configured with 6 hosts in the following config:\r\nCLC/WS/CC, SC, 4 NCs.\r\nDid BfEBS instance runs (10 instances at a time) with EMC, Netapp, and Equallogic backends configured between runs.\r\n\r\nAlso ran similar tests on a 2-node setup with a unified frontend and the EMC, Netapp, and Equallogic backends configured (one a time).\r\n\r\nAdditional tests to run: scale tests with real NCs up to 10 NCs per cluster. Overlay and DASManager tests for regressions.\r\n\r\nSummary of the fix:\r\n\r\nChange code in the SC and CC.\r\n\r\nThe SC: AttachVolume() when sent a list of IQNs to authorize will return a map of IQN->LUN since it is now possible that each IQN sees a different LUN (currently on when using the VNX). The BlockStorage code will detect if the map contains only a single value and reduce the map to just a single value in the message sent to the CC. If multiple mappings are present the remote device string instead of a single <ip>,<store>,lun,... string is now:\r\n<ip>,<store>,<lunmap>,... where <lunmap> is of the form:\r\n<iqn1>=<lun1>|<iqn2>=<lun2>|....|<iqnN>=<lunN>\r\n\r\nThe CC parses that string and selects the IQN corresponding to the single NC that will run the BfEBS instance and replaces the map with the single LUN value. This allows the NC and VMwarebroker code to remain unchanged.\r\n\r\nThe VNX provider code handles storage groups differently and creates a single storage group for each NC/iqn rather than combining them. There will be upgrade implications here so all attached volumes should be detached before upgrade.\r\n\r\nThis is intended as a short-term fix only. The number of NCs per cluster that this fix works for is only ~5-10. The limiting issue is the message timeout for the AttachVolume() call from the CLC to the SC and the SC takes much longer to service the attach request now because it must send at least one command for each NC to the VNX and each message round-trip can be multiple seconds. The design itself will work for larger cluster sizes, but the message timeout would have to be adjusted which is currently ~20 seconds and is not user-modifiable.\r\n\r\nLarger changes in how BfEBS is done are coming for 3.3 that will make this no longer required and should remove the requirement that all NCs be pre-authorized prior to BfEBS instance run message being sent to the CC.","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=zhill","name":"zhill","key":"zhill","accountId":"557058:70b4b768-eb1f-43a3-a225-3353d0fe3f56","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/239165321c2f152424c9be883721791a?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26ownerId%3Dzhill%26avatarId%3D10406%26noRedirect%3Dtrue"},"displayName":"Zach Hill","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-03-04T17:46:55.171-0600","updated":"2013-03-04T17:50:16.498-0600"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/42265","id":"42265","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"body":"Launched multiple bfebs instances per zone/cluster in a multi zone/cluster environment using EMC in each zone/cluster. Verified the device appears on the guest and small read/writes of data, and md5sums of the data. \r\n","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=mclark","name":"mclark","key":"mclark","accountId":"557058:9b68e4d6-90d8-47d3-9c86-519770009164","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/4227b07e7ea0ea725670fc4e987ed04d?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Matt Clark","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-03-25T17:05:14.177-0500","updated":"2013-03-25T17:05:14.177-0500"},{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/18917/comment/42793","id":"42793","author":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"body":"3.2.2 has been released, moving this to Resolved. ","updateAuthor":{"self":"https://eucalyptus.atlassian.net/rest/api/2/user?username=awithrow","name":"awithrow","key":"awithrow","accountId":"557058:489310fd-31eb-49f8-8552-e87970faf51b","avatarUrls":{"48x48":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=48&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3FavatarId%3D10122%26noRedirect%3Dtrue","24x24":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=24&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dsmall%26avatarId%3D10122%26noRedirect%3Dtrue","16x16":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=16&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dxsmall%26avatarId%3D10122%26noRedirect%3Dtrue","32x32":"https://avatar-cdn.atlassian.com/a5a27d1c4012a61174df794fd8406eb5?s=32&d=https%3A%2F%2Feucalyptus.atlassian.net%2Fsecure%2Fuseravatar%3Fsize%3Dmedium%26avatarId%3D10122%26noRedirect%3Dtrue"},"displayName":"Aaron Withrow","active":false,"timeZone":"America/Los_Angeles"},"created":"2013-04-03T11:02:14.140-0500","updated":"2013-04-03T11:02:14.140-0500"}],"maxResults":14,"total":14,"startAt":0},"votes":{"self":"https://eucalyptus.atlassian.net/rest/api/2/issue/EUCA-4872/votes","votes":0,"hasVoted":false},"customfield_11808":null,"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]}}}